Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{OpenAI2017,
author = {OpenAI},
title = {{OpenAI Baselines: ACKTR {\&} A2C}},
url = {https://blog.openai.com/baselines-acktr-a2c/},
urldate = {2019-03-02},
year = {2017}
}
@inproceedings{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adria Puigdomenech Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
eprint = {1602.01783},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning(2).pdf:pdf},
month = {feb},
pages = {1928--1937},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://proceedings.mlr.press/v48/mniha16.html http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@inproceedings{Li2017,
abstract = {One of the major drawbacks of modularized task-completion dialogue systems is that each module is trained individually, which presents several challenges. For example, downstream modules are affected by earlier modules, and the performance of the entire system is not robust to the accumulated errors. This paper presents a novel end-to-end learning framework for task-completion dialogue systems to tackle such issues. Our neural dialogue system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. The reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of the dialogue system. Our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines for both objective and subjective evaluation, but also is robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to the language understanding module.},
address = {Taipei, Taiwan },
archivePrefix = {arXiv},
arxivId = {1703.01008},
author = {Li, Xiujun and Chen, Yun-Nung and Li, Lihong and Gao, Jianfeng and Celikyilmaz, Asli},
booktitle = {8th International Joint Conference on Natural Language Processing},
eprint = {1703.01008},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2017 - End-to-End Task-Completion Neural Dialogue Systems.pdf:pdf},
month = {mar},
pages = {733--743 },
publisher = {Asian Federation of Natural Language Processing },
title = {{End-to-End Task-Completion Neural Dialogue Systems}},
url = {http://arxiv.org/abs/1703.01008},
year = {2017}
}
@article{Gloye2005,
abstract = {This paper shows how an omnidirectional robot can learn to correct inaccuracies when driving, or even learn to use corrective motor commands when a motor fails, whether partially or completely. Driving inaccuracies are unavoidable, since not all wheels have the same grip on the surface, or not all motors can provide exactly the same power. When a robot starts driving, the real system response differs from the ideal behavior assumed by the control software. Also, malfunctioning motors are a fact of life that we have to take into account. Our approach is to let the control software learn how the robot reacts to instructions sent from the control computer. We use a neural network, or a linear model for learning the robot's response to the commands. The model can be used to predict deviations from the desired path, and take corrective action in advance, thus increasing the driving accuracy of the robot. The model can also be used to monitor the robot and assess if it is performing according to its learned response function. If it is not, the new response function of the malfunctioning robot can be learned and updated. We show, that even if a robot loses power from a motor, the system can re-learn to drive the robot in a straight path, even if the robot is a black-box and we are not aware of how the commands are applied internally.},
author = {Gloye, Alexander and Wiesel, Fabian and Tenchio, Oliver and Simon, Mark},
doi = {10.1524/itit.2005.47.5_2005.250},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gloye et al. - 2005 - Reinforcing the Driving Quality of Soccer Playing Robots by Anticipation (Verbesserung der Fahreigenschaften von.pdf:pdf},
issn = {2196-7032},
journal = {it - Information Technology},
month = {jan},
number = {5},
pages = {250--257},
publisher = {De Gruyter Oldenbourg},
title = {{Reinforcing the Driving Quality of Soccer Playing Robots by Anticipation (Verbesserung der Fahreigenschaften von fu{\ss}ballspielenden Robotern durch Antizipation)}},
url = {http://www.degruyter.com/view/j/itit.2005.47.issue-5/itit.2005.47.5{\_}2005.250/itit.2005.47.5{\_}2005.250.xml},
volume = {47},
year = {2005}
}
@article{Silver2017a,
abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computational science,Computer science,Reward},
month = {oct},
number = {7676},
pages = {354--359},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
url = {http://www.nature.com/doifinder/10.1038/nature24270},
volume = {550},
year = {2017}
}
@incollection{Munos2016,
author = {Munos, Remi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
booktitle = {Advances in Neural Information Processing Systems 29},
pages = {1054--1062},
publisher = {Curran Associates, Inc.},
title = {{Safe and Efficient Off-Policy Reinforcement Learning}},
url = {http://papers.nips.cc/paper/6538-safe-and-efficient-off-policy-reinforcement-learning.pdf},
year = {2016}
}
@inproceedings{VanSeijen2009,
abstract = {This paper presents a theoretical and empirical analysis of Expected Sarsa, a variation on Sarsa, the classic on-policy temporal-difference method for model-free reinforcement learning. Expected Sarsa exploits knowledge about stochasticity in the behavior policy to perform updates with lower variance. Doing so allows for higher learning rates and thus faster learning. In deterministic environments, Expected Sarsas updates have zero variance, enabling a learning rate of 1. We prove that Expected Sarsa converges under the same conditions as Sarsa and formulate specific hypotheses about when Expected Sarsa will outperform Sarsa and Q-learning. Experiments in multiple domains confirm these hypotheses and demonstrate that Expected Sarsa has significant advantages over these more commonly used methods.},
author = {van Seijen, H and van Hasselt, H and Whiteson, S and Wiering, M},
booktitle = {2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning},
doi = {10.1109/ADPRL.2009.4927542},
issn = {2325-1824},
keywords = {Artificial intelligence,Convergence,Dynamic programming,Intelligent systems,Optimal control,Probability distribution,Robot control,State estimation,State feedback,Supervised learning,behavior policy,deterministic environment,expected Sarsa analysis,learning (artificial intelligence),model-free reinforcement learning,on-policy temporal-difference method,stochastic processes,stochasticity,zero variance},
month = {mar},
pages = {177--184},
title = {{A theoretical and empirical analysis of Expected Sarsa}},
year = {2009}
}
@article{Tesauro1994,
abstract = {TD-Gammon is a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results, based on the TD($\lambda$) reinforcement learning algorithm (Sutton 1988). Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in at the start of learning (i.e., given only a “raw” description of the board state), the network learns to play at a strong intermediate level. Furthermore, when a set of hand-crafted features is added to the network's input representation, the result is a truly staggering level of performance: the latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world's best human players.},
address = {Boston, MA},
author = {Tesauro, Gerald},
doi = {10.1162/neco.1994.6.2.215},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Networks, 1995 - 1993 - Td-gammon A self-teaching backgammon program.pdf:pdf},
isbn = {978-1-4757-2379-3},
journal = {Applications of Neural Networks},
number = {2},
pages = {215--219},
publisher = {Springer US},
title = {{TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play}},
url = {https://doi.org/10.1162/neco.1994.6.2.215},
volume = {6},
year = {1994}
}
@article{Assael2015,
abstract = {Data-efficient reinforcement learning (RL) in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems. We consider a particularly important instance of this challenge, the pixels-to-torques problem, where an RL agent learns a closed-loop control policy ("torques") from pixel information only. We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information. The key ingredient is a deep dynamical model for learning a low-dimensional feature embedding of images jointly with a predictive model in this low-dimensional feature space. Joint learning is crucial for long-term predictions, which lie at the core of the adaptive nonlinear model predictive control strategy that we use for closed-loop control. Compared to state-of-the-art RL methods for continuous states and actions, our approach learns quickly, scales to high-dimensional state spaces, is lightweight and an important step toward fully autonomous end-to-end learning from pixels to torques.},
archivePrefix = {arXiv},
arxivId = {1510.02173},
author = {Assael, John-Alexander M. and Wahlstr{\"{o}}m, Niklas and Sch{\"{o}}n, Thomas B. and Deisenroth, Marc Peter},
eprint = {1510.02173},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Assael et al. - 2015 - Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models.pdf:pdf},
month = {oct},
title = {{Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models}},
url = {http://arxiv.org/abs/1510.02173},
year = {2015}
}
@inproceedings{Schulman2015a,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
archivePrefix = {arXiv},
arxivId = {1506.02438},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1506.02438},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2015 - High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf:pdf},
month = {jun},
title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
url = {http://arxiv.org/abs/1506.02438},
year = {2015}
}
@inproceedings{Kakade2002,
author = {Kakade, Sham M},
booktitle = {Advances in neural information processing systems},
pages = {1531--1538},
title = {{A natural policy gradient}},
year = {2002}
}
@article{Haarnoja2018,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
archivePrefix = {arXiv},
arxivId = {1812.05905},
author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
eprint = {1812.05905},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haarnoja et al. - 2018 - Soft Actor-Critic Algorithms and Applications.pdf:pdf},
month = {dec},
title = {{Soft Actor-Critic Algorithms and Applications}},
url = {http://arxiv.org/abs/1812.05905},
year = {2018}
}
@article{Jaderberg2018,
abstract = {Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.},
archivePrefix = {arXiv},
arxivId = {1807.01281},
author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
eprint = {1807.01281},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2018 - Human-level performance in first-person multiplayer games with population-based deep reinforcement learning.pdf:pdf},
month = {jul},
title = {{Human-level performance in first-person multiplayer games with population-based deep reinforcement learning}},
url = {http://arxiv.org/abs/1807.01281},
year = {2018}
}
@article{Bohg2017,
abstract = {Recent approaches in robotics follow the insight that perception is facilitated by interaction with the environment. These approaches are subsumed under the term of Interactive Perception (IP). It provides the following benefits: (i) interaction with the environment creates a rich sensory signal that would otherwise not be present and (ii) knowledge of the regularity in the combined space of sensory data and action parameters facilitate the prediction and interpretation of the signal. In this survey we postulate this as a principle and collect evidence in support by analyzing and categorizing existing work in this area. We also provide an overview of the most important applications of Interactive Perception. We close this survey by discussing remaining open questions. Thereby, we hope to define a field and inspire future work.},
archivePrefix = {arXiv},
arxivId = {1604.03670},
author = {Bohg, Jeannette and Hausman, Karol and Sankaran, Bharath and Brock, Oliver and Kragic, Danica and Schaal, Stefan and Sukhatme, Gaurav},
doi = {10.1109/TRO.2017.2721939},
eprint = {1604.03670},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bohg et al. - 2017 - Interactive Perception Leveraging Action in Perception and Perception in Action.pdf:pdf},
journal = {IEEE Transactions on Robotics},
month = {dec},
number = {6},
pages = {1273--1291},
title = {{Interactive Perception: Leveraging Action in Perception and Perception in Action}},
url = {http://arxiv.org/abs/1604.03670},
volume = {33},
year = {2017}
}
@article{Andrychowicz2018,
abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM},
archivePrefix = {arXiv},
arxivId = {1808.00177},
author = {Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
eprint = {1808.00177},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrychowicz et al. - 2018 - Learning Dexterous In-Hand Manipulation(2).pdf:pdf},
month = {aug},
title = {{Learning Dexterous In-Hand Manipulation}},
url = {http://arxiv.org/abs/1808.00177},
year = {2018}
}
@inproceedings{Nair2015a,
abstract = {We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.},
address = {Lille, France},
archivePrefix = {arXiv},
arxivId = {1507.04296},
author = {Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and {De Maria}, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and Legg, Shane and Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David},
booktitle = {Deep Learning Workshop, International Conference on Machine Learning},
eprint = {1507.04296},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nair et al. - 2015 - Massively Parallel Methods for Deep Reinforcement Learning(2).pdf:pdf},
month = {jul},
title = {{Massively Parallel Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1507.04296},
year = {2015}
}
@inproceedings{Vezhnevets2017,
abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
archivePrefix = {arXiv},
arxivId = {1703.01161},
author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
booktitle = {International Conference on Machine Learning},
eprint = {1703.01161},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vezhnevets et al. - 2017 - FeUdal Networks for Hierarchical Reinforcement Learning.pdf:pdf},
month = {mar},
title = {{FeUdal Networks for Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.01161},
year = {2017}
}
@article{Mnih2015,
abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computer science},
month = {feb},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://www.nature.com/articles/nature14236},
volume = {518},
year = {2015}
}
@inproceedings{Schulman2015,
abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
address = {Lille, France},
annote = {Trust Region Policy Optimization. This method, introduced by Schulman et al. (2015), uses a bound on the expected Kullback-Leibler divergence between successive policies, inspired by a theoretical policy iteration algorithm that guarantees non-decreasing expected returns. The method, TRPO, requires a parametric policy to be defined, but has been shown to work well with deep neural-network policies that can be used in many different tasks thanks to their flexibility. We use the reference implementation provided in the RLlab framework (Duan et al., 2016). This TRPO implementation requires a fixed time horizon, so we evaluate this model on a modified version of the tasks where the episodes are of fixed length, equal to the expected episode length used for other methods.},
author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
booktitle = {Proc. 32nd Int. Conf. Mach. Learn.},
file = {:home/fabian/Desktop/Trust Region Policy Optimization.pdf:pdf},
pages = {1889--1897},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Trust Region Policy Optimization}},
url = {http://proceedings.mlr.press/v37/schulman15.html},
volume = {37},
year = {2015}
}
@inproceedings{Lillicrap2016,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
address = {London, UK},
annote = {From Duplicate 1 (Continuous control with deep reinforcement learning - Lillicrap, Timothy P.; Hunt, Jonathan J.; Pritzel, Alexander; Heess, Nicolas; Erez, Tom; Tassa, Yuval; Silver, David; Wierstra, Daan)

Deep Deterministic Policy Gradients. Lillicrap et al. (2016) introduced a policy gradient algorithm, DDPG, that works with deterministic policies. This is an actor-critic method that uses neural networks for both the Q function and the policy. After each time step, the new experience sample is stored in an ‘experience replay' buffer, a randomly sampled batch from this buffer is then used to calculate the policy gradient. This technique diminishes the problem that successive samples tend to be highly correlated. We again used the RLlab implementation (Duan et al., 2016) with fixed episode length.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
booktitle = {International Conference on Learning Representations (ICLR) 2016},
eprint = {1509.02971},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learning.pdf:pdf},
month = {sep},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2016}
}
@inproceedings{Caicedo2015,
abstract = {We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.},
address = {Washington, DC, USA},
archivePrefix = {arXiv},
arxivId = {1511.06015},
author = {Caicedo, Juan C. and Lazebnik, Svetlana},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.286},
eprint = {1511.06015},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caicedo, Lazebnik - 2015 - Active Object Localization with Deep Reinforcement Learning.pdf:pdf},
month = {nov},
pages = {2488--2496},
publisher = {IEEE},
title = {{Active Object Localization with Deep Reinforcement Learning}},
url = {https://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/papers/Caicedo{\_}Active{\_}Object{\_}Localization{\_}ICCV{\_}2015{\_}paper.pdf http://arxiv.org/abs/1511.06015},
year = {2015}
}
@inproceedings{Schaul2015,
abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
archivePrefix = {arXiv},
arxivId = {1511.05952},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
booktitle = {Interantional Conference for Learning Representations (ICLR)},
eprint = {1511.05952},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaul et al. - 2015 - Prioritized Experience Replay.pdf:pdf},
month = {nov},
title = {{Prioritized Experience Replay}},
url = {http://arxiv.org/abs/1511.05952},
year = {2015}
}
@incollection{Sontag1993,
address = {Boston, MA},
author = {Sontag, Eduardo D.},
booktitle = {Essays on Control},
doi = {10.1007/978-1-4612-0313-1_10},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sontag - 1993 - Neural Networks for Control.pdf:pdf},
pages = {339--380},
publisher = {Birkh{\"{a}}user Boston},
title = {{Neural Networks for Control}},
url = {http://link.springer.com/10.1007/978-1-4612-0313-1{\_}10},
year = {1993}
}
@article{Babaeizadeh2017,
abstract = {We introduce a hybrid CPU/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. We analyze its computational traits and concentrate on aspects critical to leveraging the GPU's computational power. We introduce a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. Our hybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant speed up compared to a CPU implementation; we make it publicly available to other researchers at https://github.com/NVlabs/GA3C .},
archivePrefix = {arXiv},
arxivId = {1611.06256},
author = {Babaeizadeh, Mohammad and Frosio, Iuri and Tyree, Stephen and Clemons, Jason and Kautz, Jan},
eprint = {1611.06256},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Babaeizadeh et al. - 2016 - Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU.pdf:pdf},
month = {nov},
title = {{Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU}},
url = {http://arxiv.org/abs/1611.06256},
year = {2017}
}
@article{Rusu2016,
abstract = {Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1610.04286},
author = {Rusu, Andrei A. and Vecerik, Mel and Roth{\"{o}}rl, Thomas and Heess, Nicolas and Pascanu, Razvan and Hadsell, Raia},
eprint = {1610.04286},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rusu et al. - 2016 - Sim-to-Real Robot Learning from Pixels with Progressive Nets.pdf:pdf},
month = {oct},
title = {{Sim-to-Real Robot Learning from Pixels with Progressive Nets}},
url = {http://arxiv.org/abs/1610.04286},
year = {2016}
}
@inproceedings{7989379,
abstract = {Policy search can in principle acquire complex strategies for control of robots and other autonomous systems. When the policy is trained to process raw sensory inputs, such as images and depth maps, it can also acquire a strategy that combines perception and control. However, effectively processing such complex inputs requires an expressive policy class, such as a large neural network. These high-dimensional policies are difficult to train, especially when learning to control safety-critical systems. We propose PLATO, a continuous, reset-free reinforcement learning algorithm that trains complex control policies with supervised learning, using model-predictive control (MPC) to generate the supervision, hence never in need of running a partially trained and potentially unsafe policy. PLATO uses an adaptive training method to modify the behavior of MPC to gradually match the learned policy in order to generate training samples at states that are likely to be visited by the learned policy. PLATO also maintains the MPC cost as an objective to avoid highly undesirable actions that would result from strictly following the learned policy before it has been fully trained. We prove that this type of adaptive MPC expert produces supervision that leads to good long-horizon performance of the resulting policy. We also empirically demonstrate that MPC can still avoid dangerous on-policy actions in unexpected situations during training. Our empirical results on a set of challenging simulated aerial vehicle tasks demonstrate that, compared to prior methods, PLATO learns faster, experiences substantially fewer catastrophic failures (crashes) during training, and often converges to a better policy.},
author = {Kahn, G and Zhang, T and Levine, S and Abbeel, P},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989379},
file = {:home/fabian/Desktop/PLATO$\backslash$: Policy Learning using Adaptive Trajectory Optimization.pdf:pdf},
keywords = {Learning (artificial intelligence),Neural networks,PLATO,Robots,Robustness,Supervised learning,Training,Trajectory optimization,adaptive MPC,adaptive control,adaptive training method,aerospace robotics,autonomous systems,complex control policies,continuous reset-free reinforcement learning algor,continuous systems,large-scale systems,learning systems,model-predictive control,policy learning using adaptive trajectory optimiza,policy search,predictive control,robots,simulated aerial vehicle tasks,supervised learning,trajectory optimisation (aerospace)},
month = {may},
pages = {3342--3349},
title = {{PLATO: Policy learning using adaptive trajectory optimization}},
year = {2017}
}
@inproceedings{OKelly2018,
abstract = {While recent developments in autonomous vehicle (AV) technology highlight substantial progress, we lack tools for rigorous and scalable testing. Real-world testing, the {\$}\backslashtextit{\{}de facto{\}}{\$} evaluation environment, places the public in danger, and, due to the rare nature of accidents, will require billions of miles in order to statistically validate performance claims. We implement a simulation framework that can test an entire modern autonomous driving system, including, in particular, systems that employ deep-learning perception and control algorithms. Using adaptive importance-sampling methods to accelerate rare-event probability evaluation, we estimate the probability of an accident under a base distribution governing standard traffic behavior. We demonstrate our framework on a highway scenario, accelerating system evaluation by {\$}2{\$}-{\$}20{\$} times over naive Monte Carlo sampling methods and {\$}10{\$}-{\$}300 \backslashmathsf{\{}P{\}}{\$} times (where {\$}\backslashmathsf{\{}P{\}}{\$} is the number of processors) over real-world testing.},
archivePrefix = {arXiv},
arxivId = {1811.00145},
author = {O'Kelly, Matthew and Sinha, Aman and Namkoong, Hongseok and Duchi, John and Tedrake, Russ},
booktitle = {Conference on Neural Information Processing Systems (NIPS)},
eprint = {1811.00145},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Kelly et al. - 2018 - Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation.pdf:pdf},
month = {oct},
title = {{Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation}},
url = {http://arxiv.org/abs/1811.00145},
year = {2018}
}
@article{Abdolmaleki2018,
abstract = {We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings while achieving similar or better final performance.},
archivePrefix = {arXiv},
arxivId = {1806.06920},
author = {Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
eprint = {1806.06920},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abdolmaleki et al. - 2018 - Maximum a Posteriori Policy Optimisation.pdf:pdf},
month = {jun},
title = {{Maximum a Posteriori Policy Optimisation}},
url = {http://arxiv.org/abs/1806.06920},
year = {2018}
}
@article{Haarnoja2017,
abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
archivePrefix = {arXiv},
arxivId = {1702.08165},
author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
eprint = {1702.08165},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haarnoja et al. - 2017 - Reinforcement Learning with Deep Energy-Based Policies.pdf:pdf},
month = {feb},
title = {{Reinforcement Learning with Deep Energy-Based Policies}},
url = {http://arxiv.org/abs/1702.08165},
year = {2017}
}
@article{Meeden1993,
author = {Meeden, Lisa and Meeden, Lisa and Mcgraw, Gary and Blank, Douglas},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meeden et al. - 1993 - Emergent Control and Planning in an Autonomous Vehicle.pdf:pdf},
journal = {Proceedings of the Fifteenth Annual Meeting of the Cognitive ScienceSociety},
pages = {735--740},
title = {{Emergent Control and Planning in an Autonomous Vehicle}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.3932},
year = {1993}
}
@article{Deisenroth:2013:SPS:2688186.2688187,
address = {Hanover, MA, USA},
author = {Deisenroth, Marc Peter and Neumann, Gerhard and Peters, Jan},
doi = {10.1561/2300000021},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deisenroth, Neumann, Peters - 2013 - A Survey on Policy Search for Robotics.pdf:pdf},
issn = {1935-8253},
journal = {Foundations and Trends in Robotics},
month = {aug},
number = {1-2},
pages = {1--142},
publisher = {Now Publishers Inc.},
title = {{A Survey on Policy Search for Robotics}},
url = {http://dx.doi.org/10.1561/2300000021},
volume = {2},
year = {2013}
}
@article{Silver2017,
abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
archivePrefix = {arXiv},
arxivId = {1712.01815},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
doi = {10.1002/acn3.501},
eprint = {1712.01815},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.pdf:pdf},
isbn = {3013372370},
issn = {23289503},
month = {dec},
pages = {1--19},
pmid = {29376095},
title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}},
url = {http://arxiv.org/abs/1712.01815},
year = {2017}
}
@article{Tampuu2017,
abstract = {Evolution of cooperation and competition can appear when multiple adaptive agents share a biological, social, or technological niche. In the present work we study how cooperation and competition emerge between autonomous agents that learn by reinforcement while using only their raw visual input as the state representation. In particular, we extend the Deep Q-Learning framework to multiagent environments to investigate the interaction between two learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also describe the progression from competitive to collaborative behavior when the incentive to cooperate is increased. Finally we show how learning by playing against another adaptive agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized learning of multiagent systems coping with high-dimensional environments.},
author = {Tampuu, Ardi and Matiisen, Tambet and Kodelja, Dorian and Kuzovkin, Ilya and Korjus, Kristjan and Aru, Juhan and Aru, Jaan and Vicente, Raul},
doi = {10.1371/journal.pone.0172395},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tampuu et al. - 2017 - Multiagent cooperation and competition with deep reinforcement learning.pdf:pdf},
issn = {1932-6203},
journal = {PLOS ONE},
month = {apr},
number = {4},
pages = {e0172395},
publisher = {Public Library of Science},
title = {{Multiagent cooperation and competition with deep reinforcement learning}},
url = {http://dx.plos.org/10.1371/journal.pone.0172395},
volume = {12},
year = {2017}
}
@inproceedings{Lee2017,
abstract = {Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual{\_}servoing .},
archivePrefix = {arXiv},
arxivId = {1703.11000},
author = {Lee, Alex X. and Levine, Sergey and Abbeel, Pieter},
booktitle = {5th International Conference on Learning Representations (ICLR)},
eprint = {1703.11000},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Levine, Abbeel - 2017 - Learning Visual Servoing with Deep Features and Fitted Q-Iteration.pdf:pdf},
month = {mar},
title = {{Learning Visual Servoing with Deep Features and Fitted Q-Iteration}},
url = {http://arxiv.org/abs/1703.11000},
year = {2017}
}
@inproceedings{Gu2017,
author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989385},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2017 - Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates.pdf:pdf},
isbn = {978-1-5090-4633-1},
month = {may},
pages = {3389--3396},
publisher = {IEEE},
title = {{Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates}},
url = {http://ieeexplore.ieee.org/document/7989385/},
year = {2017}
}
@inproceedings{Silver2014,
address = {Beijing, China},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:home/fabian/Desktop/Deterministic Policy Gradient Algorithms.pdf:pdf},
pages = {I--387----I--395},
publisher = {JMLR.org},
series = {ICML'14},
title = {{Deterministic Policy Gradient Algorithms}},
url = {http://dl.acm.org/citation.cfm?id=3044805.3044850},
year = {2014}
}
@article{Gruslys2017,
abstract = {In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the $\backslash$b{\{}eta{\}}-leave-one-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.},
archivePrefix = {arXiv},
arxivId = {1704.04651},
author = {Gruslys, Audrunas and Dabney, Will and Azar, Mohammad Gheshlaghi and Piot, Bilal and Bellemare, Marc and Munos, Remi},
eprint = {1704.04651},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gruslys et al. - 2017 - The Reactor A fast and sample-efficient Actor-Critic agent for Reinforcement Learning.pdf:pdf},
month = {apr},
title = {{The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning}},
url = {http://arxiv.org/abs/1704.04651},
year = {2017}
}
@article{Burda2018,
abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
archivePrefix = {arXiv},
arxivId = {1810.12894},
author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
eprint = {1810.12894},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burda et al. - 2018 - Exploration by Random Network Distillation.pdf:pdf},
month = {oct},
title = {{Exploration by Random Network Distillation}},
url = {http://arxiv.org/abs/1810.12894},
year = {2018}
}
@incollection{Hausknecht2015,
abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting $\backslash$textit{\{}Deep Recurrent Q-Network{\}} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew and Stone, Peter},
booktitle = {AAAI Fall Symposia},
eprint = {1507.06527},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hausknecht, Stone - 2015 - Deep Recurrent Q-Learning for Partially Observable MDPs.pdf:pdf},
month = {jul},
title = {{Deep Recurrent Q-Learning for Partially Observable MDPs}},
url = {http://arxiv.org/abs/1507.06527},
year = {2015}
}
@inproceedings{Todorov2005,
abstract = {We present an iterative linear-quadratic-Gaussian method for locally-optimal feedback control of nonlinear stochastic systems subject to control constraints. Previously, similar methods have been restricted to deterministic unconstrained problems with quadratic costs. The new method constructs an affine feedback control law, obtained by minimizing a novel quadratic approximation to the optimal cost-to-go function. Global convergence is guaranteed through a Levenberg-Marquardt method; convergence in the vicinity of a local minimum is quadratic. Performance is illustrated on a limited-torque inverted pendulum problem, as well as a complex biomechanical control problem involving a stochastic model of the human arm, with 10 state dimensions and 6 muscle actuators. A Matlab implementation of the new algorithm is availabe at www.cogsci.ucsd.edu//spl sim/todorov.},
author = {Todorov, E and Li, Weiwei},
booktitle = {American Control Conference},
doi = {10.1109/ACC.2005.1469949},
issn = {0743-1619},
keywords = {Control systems,Convergence,Costs,Feedback control,Iterative methods,Levenberg-Marquardt method,Linear feedback control systems,Mathematical model,Matlab,Nonlinear control systems,Stochastic processes,Stochastic systems,actuators,biomechanical control problem,biomechanics,constrained nonlinear stochastic systems,constraint theory,control constraints,convergence,deterministic unconstrained problems,feedback,generalized iterative LQG method,global convergence,human arm,iterative linear-quadratic-Gaussian method,iterative methods,limited-torque inverted pendulum,linear quadratic Gaussian control,locally-optimal feedback control,muscle,muscle actuators,nonlinear systems,optimal control,optimal cost-to-go function,pendulums,quadratic approximation,quadratic cost,quadratic programming,state dimensions,stochastic model,stochastic systems},
month = {jun},
pages = {300--306 vol. 1},
title = {{A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems}},
year = {2005}
}
@inproceedings{GuillaumeLample2016,
abstract = {Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as average humans in deathmatch scenarios.},
author = {{Guillaume Lample} and {Devendra Singh Chaplot}},
booktitle = {AAAI Conference on Artificial Intelligence},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guillaume Lample, Devendra Singh Chaplot - 2016 - Playing FPS Games with Deep Reinforcement Learning.pdf:pdf},
pages = {2140--2146},
title = {{Playing FPS Games with Deep Reinforcement Learning}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14456/14385},
year = {2016}
}
@inproceedings{Nagabandi2018,
author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey},
booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2018.8463189},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagabandi et al. - 2018 - Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning.pdf:pdf},
isbn = {978-1-5386-3081-5},
month = {may},
pages = {7559--7566},
publisher = {IEEE},
title = {{Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning}},
url = {https://ieeexplore.ieee.org/document/8463189/},
year = {2018}
}
@inproceedings{Yun2017,
author = {Yun, Sangdoo and Choi, Jongwon and Yoo, Youngjoon and Yun, Kimin and Choi, Jin Young},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.148},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yun et al. - 2017 - Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {1349--1358},
publisher = {IEEE},
title = {{Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning}},
url = {http://ieeexplore.ieee.org/document/8099631/},
year = {2017}
}
@article{Jaderberg2016,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880$\backslash${\%} expert human performance, and a challenging suite of first-person, three-dimensional $\backslash$emph{\{}Labyrinth{\}} tasks leading to a mean speedup in learning of 10{\$}\backslashtimes{\$} and averaging 87$\backslash${\%} expert human performance on Labyrinth.},
archivePrefix = {arXiv},
arxivId = {1611.05397},
author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
eprint = {1611.05397},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2016 - Reinforcement Learning with Unsupervised Auxiliary Tasks.pdf:pdf},
month = {nov},
title = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
url = {http://arxiv.org/abs/1611.05397},
year = {2016}
}
@article{Sontag1993a,
author = {Sontag, Eduardo D.},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sontag - 1993 - Some Topics in Neural Networks and Control.pdf:pdf},
journal = {IN PROCEEDINGS OF THE EUROPEAN CONTROL CONFERENCE},
title = {{Some Topics in Neural Networks and Control}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.9748},
year = {1993}
}
@inproceedings{Haarnoja2018b,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
booktitle = {International Conference for Machine Learning (ICML)},
eprint = {1801.01290},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:pdf},
month = {jan},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {http://arxiv.org/abs/1801.01290},
year = {2018}
}
@article{Weber2017,
abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
archivePrefix = {arXiv},
arxivId = {1707.06203},
author = {Weber, Th{\'{e}}ophane and Racani{\`{e}}re, S{\'{e}}bastien and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdom{\`{e}}nech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
eprint = {1707.06203},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weber et al. - 2017 - Imagination-Augmented Agents for Deep Reinforcement Learning.pdf:pdf},
month = {jul},
title = {{Imagination-Augmented Agents for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1707.06203},
year = {2017}
}
@article{Lange2010,
abstract = {This paper discusses the effectiveness of deep auto-encoder neural networks in visual reinforcement learning (RL) tasks. We propose a framework for combining the training of deep auto-encoders (for learning compact feature spaces) with recently-proposed batch-mode RL algorithms (for learning policies). An emphasis is put on the data-efficiency of this combination and on studying the properties of the feature spaces automatically constructed by the deep auto-encoders. These feature spaces are empirically shown to adequately resemble existing similarities and spatial relations between observations and allow to learn useful policies. We propose several methods for improving the topology of the feature spaces making use of task-dependent information. Finally, we present first results on successfully learning good control policies directly on synthesized and real images.},
archivePrefix = {arXiv},
arxivId = {1507.04296},
author = {Lange, Sascha and Riedmiller, Martin},
doi = {10.1109/IJCNN.2010.5596468},
eprint = {1507.04296},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lange, Riedmiller - 2010 - Deep auto-encoder neural networks in reinforcement learning.pdf:pdf},
isbn = {9781424469178},
issn = {1098-7576},
journal = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
pages = {1--9},
pmid = {25719670},
title = {{Deep auto-encoder neural networks in reinforcement learning}},
volume = {2600},
year = {2010}
}
@inproceedings{Zhang2017,
abstract = {In this paper we consider the problem of robot navigation in simple maze-like environments where the robot has to rely on its onboard sensors to perform the navigation task. In particular, we are interested in solutions to this problem that do not require localization, mapping or planning. Additionally, we require that our solution can quickly adapt to new situations (e.g., changing navigation goals and environments). To meet these criteria we frame this problem as a sequence of related reinforcement learning tasks. We propose a successor feature based deep reinforcement learning algorithm that can learn to transfer knowledge from previously mastered navigation tasks to new problem instances. Our algorithm substantially decreases the required learning time after the first task instance has been solved, which makes it easily adaptable to changing environments. We validate our method in both simulated and real robot experiments with a Robotino and compare it to a set of baseline methods including classical planning-based navigation.},
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {1612.05533},
author = {Zhang, Jingwei and Springenberg, Jost Tobias and Boedecker, Joschka and Burgard, Wolfram},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2017.8206049},
eprint = {1612.05533},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Deep Reinforcement Learning with Successor Features for Navigation across Similar Environments.pdf:pdf},
month = {dec},
pages = {2371--2378},
title = {{Deep Reinforcement Learning with Successor Features for Navigation across Similar Environments}},
url = {http://arxiv.org/abs/1612.05533},
year = {2017}
}
@inproceedings{Bakker2003a,
author = {Bakker, B. and Zhumatiy, V. and Gruener, G. and Schmidhuber, J.},
booktitle = {Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)},
doi = {10.1109/IROS.2003.1250667},
isbn = {0-7803-7860-1},
pages = {430--435},
publisher = {IEEE},
title = {{A robot that reinforcement-leams to identify and memorize important previous observations}},
url = {http://ieeexplore.ieee.org/document/1250667/},
volume = {1},
year = {2003}
}
@inproceedings{Gu2016,
abstract = {Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1603.00748},
author = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
booktitle = {ICML'16 Proceedings of the 33rd International Conference on International Conference onMachine Learning - Volume 48},
eprint = {1603.00748},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2016 - Continuous Deep Q-Learning with Model-based Acceleration.pdf:pdf},
month = {mar},
pages = {2829--2838},
publisher = {JMLR.org},
title = {{Continuous Deep Q-Learning with Model-based Acceleration}},
url = {http://arxiv.org/abs/1603.00748},
year = {2016}
}
@article{Foerster2016,
abstract = {We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.},
archivePrefix = {arXiv},
arxivId = {1602.02672},
author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
eprint = {1602.02672},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Foerster et al. - 2016 - Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks.pdf:pdf},
month = {feb},
title = {{Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks}},
url = {http://arxiv.org/abs/1602.02672},
year = {2016}
}
@inproceedings{Jouppi2017,
address = {New York, NY, USA},
author = {Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
booktitle = {International Symposium on Computer Architecture (ISCA)},
doi = {10.1145/3079856.3080246},
isbn = {978-1-4503-4892-8},
keywords = {CNN,DNN,GPU,LSTM,MLP,RNN,TPU,TensorFlow,accelerator,deep learning,domain-specific architecture,neural network},
pages = {1--12},
publisher = {ACM},
series = {ISCA '17},
title = {{In-Datacenter Performance Analysis of a Tensor Processing Unit}},
url = {http://doi.acm.org/10.1145/3079856.3080246},
year = {2017}
}
@inproceedings{10.1007/11552246_35,
abstract = {Helicopters have highly stochastic, nonlinear, dynamics, and autonomous helicopter flight is widely regarded to be a challenging control problem. As helicopters are highly unstable at low speeds, it is particularly difficult to design controllers for low speed aerobatic maneuvers. In this paper, we describe a successful application of reinforcement learning to designing a controller for sustained inverted flight on an autonomous helicopter. Using data collected from the helicopter in flight, we began by learning a stochastic, nonlinear model of the helicopter's dynamics. Then, a reinforcement learning algorithm was applied to automatically learn a controller for autonomous inverted hovering. Finally, the resulting controller was successfully tested on our autonomous helicopter platform.},
address = {Berlin, Heidelberg},
author = {Ng, Andrew Y and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
booktitle = {Experimental Robotics IX},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng et al. - 2006 - Autonomous Inverted Helicopter Flight via Reinforcement Learning.pdf:pdf},
isbn = {978-3-540-33014-1},
pages = {363--372},
publisher = {Springer Berlin Heidelberg},
title = {{Autonomous Inverted Helicopter Flight via Reinforcement Learning}},
year = {2006}
}
@inproceedings{Chebotar2017,
abstract = {Reinforcement learning (RL) algorithms for real-world robotic applications need a data-efficient learning process and the ability to handle complex, unknown dynamical systems. These requirements are handled well by model-based and model-free RL approaches, respectively. In this work, we aim to combine the advantages of these two types of methods in a principled manner. By focusing on time-varying linear-Gaussian policies, we enable a model-based algorithm based on the linear quadratic regulator (LQR) that can be integrated into the model-free framework of path integral policy improvement (PI2). We can further combine our method with guided policy search (GPS) to train arbitrary parameterized policies such as deep neural networks. Our simulation and real-world experiments demonstrate that this method can solve challenging manipulation tasks with comparable or better performance than model-free methods while maintaining the sample efficiency of model-based methods. A video presenting our results is available at https://sites.google.com/site/icml17pilqr},
address = {Sydney, Australia},
archivePrefix = {arXiv},
arxivId = {1703.03078},
author = {Chebotar, Yevgen and Hausman, Karol and Zhang, Marvin and Sukhatme, Gaurav and Schaal, Stefan and Levine, Sergey},
booktitle = {34th International Conference on Machine Learning},
eprint = {1703.03078},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chebotar et al. - 2017 - Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning.pdf:pdf},
month = {mar},
pages = {703----711},
publisher = {PMLR},
title = {{Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.03078},
year = {2017}
}
@inproceedings{7989466,
abstract = {Learning from demonstrations has been shown to be a successful method for non-experts to teach manipulation tasks to robots. These methods typically build generative models from demonstrations and then use regression to reproduce skills. However, this approach has limitations to capture hard geometric constraints imposed by the task. On the other hand, while sampling and optimization-based motion planners exist that reason about geometric constraints, these are typically carefully hand-crafted by an expert. To address this technical gap, we contribute with C-LEARN, a method that learns multi-step manipulation tasks from demonstrations as a sequence of keyframes and a set of geometric constraints. The system builds a knowledge base for reaching and grasping objects, which is then leveraged to learn multi-step tasks from a single demonstration. C-LEARN supports multi-step tasks with multiple end effectors; reasons about SE(3) volumetric and CAD constraints, such as the need for two axes to be parallel; and offers a principled way to transfer skills between robots with different kinematics. We embed the execution of the learned tasks within a shared autonomy framework, and evaluate our approach by analyzing the success rate when performing physical tasks with a dual-arm Optimas robot, comparing the contribution of different constraints models, and demonstrating the ability of C-LEARN to transfer learned tasks by performing them with a legged dual-arm Atlas robot in simulation.},
author = {P{\'{e}}rez-D'Arpino, C and Shah, J A},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989466},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{e}}rez-D'Arpino, Shah - 2017 - C-LEARN Learning geometric constraints from demonstrations for multi-step manipulation in shared autonomy.pdf:pdf},
keywords = {C-LEARN,End effectors,Grasping,Hidden Markov models,Knowledge based systems,Planning,Solid modeling,control engineering computing,dual-arm Optimas robot,end effectors,knowledge base,knowledge based systems,learning (artificial intelligence),learning geometric constraints,legged dual-arm Atlas robot,legged locomotion,manipulator kinematics,multistep manipulation,multistep tasks,optimization-based motion planners,path planning,regression,regression analysis,robot kinematics,sampling methods,sampling-based motion planners,shared autonomy},
pages = {4058--4065},
title = {{C-LEARN: Learning geometric constraints from demonstrations for multi-step manipulation in shared autonomy}},
year = {2017}
}
@inproceedings{Riedmiller1993,
author = {Riedmiller, Martin and Braun, Heinrich},
booktitle = {IEEE International Conference on Neural Networks (IJCNN)},
pages = {586--591},
title = {{A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm}},
year = {1993}
}
@article{Bansal2017,
abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
archivePrefix = {arXiv},
arxivId = {1710.03748},
author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
eprint = {1710.03748},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bansal et al. - 2017 - Emergent Complexity via Multi-Agent Competition.pdf:pdf},
month = {oct},
title = {{Emergent Complexity via Multi-Agent Competition}},
url = {http://arxiv.org/abs/1710.03748},
year = {2017}
}
@article{Bellemare2015,
abstract = {This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.},
archivePrefix = {arXiv},
arxivId = {1512.04860},
author = {Bellemare, Marc G. and Ostrovski, Georg and Guez, Arthur and Thomas, Philip S. and Munos, R{\'{e}}mi},
eprint = {1512.04860},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellemare et al. - 2015 - Increasing the Action Gap New Operators for Reinforcement Learning.pdf:pdf},
month = {dec},
title = {{Increasing the Action Gap: New Operators for Reinforcement Learning}},
url = {http://arxiv.org/abs/1512.04860},
year = {2015}
}
@unpublished{Li2018,
abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
archivePrefix = {arXiv},
arxivId = {1810.06339},
author = {Li, Yuxi},
eprint = {1810.06339},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2018 - Deep Reinforcement Learning.pdf:pdf},
institution = {Morgan {\&} Claypool: Synthesis Lectures in Artificial Intelligence and Machine Learning},
month = {oct},
title = {{Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1810.06339},
year = {2018}
}
@article{Vinyals2017,
abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
archivePrefix = {arXiv},
arxivId = {1708.04782},
author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"{u}}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and van Hasselt, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
eprint = {1708.04782},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vinyals et al. - 2017 - StarCraft II A New Challenge for Reinforcement Learning.pdf:pdf},
month = {aug},
title = {{StarCraft II: A New Challenge for Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.04782},
year = {2017}
}
@article{Silver2016,
abstract = {A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computational science,Computer science,Reward},
month = {jan},
number = {7587},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://www.nature.com/articles/nature16961},
volume = {529},
year = {2016}
}
@article{Zambaldi2018a,
abstract = {We introduce an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. Our results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games -- surpassing human grandmaster performance on four. By considering architectural inductive biases, our work opens new directions for overcoming important, but stubborn, challenges in deep RL.},
archivePrefix = {arXiv},
arxivId = {1806.01830},
author = {Zambaldi, Vinicius and Raposo, David and Santoro, Adam and Bapst, Victor and Li, Yujia and Babuschkin, Igor and Tuyls, Karl and Reichert, David and Lillicrap, Timothy and Lockhart, Edward and Shanahan, Murray and Langston, Victoria and Pascanu, Razvan and Botvinick, Matthew and Vinyals, Oriol and Battaglia, Peter},
eprint = {1806.01830},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zambaldi et al. - 2018 - Relational Deep Reinforcement Learning.pdf:pdf},
month = {jun},
title = {{Relational Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.01830},
year = {2018}
}
@article{Sadeghi2017,
abstract = {Humans are remarkably proficient at controlling their limbs and tools from a wide range of viewpoints and angles, even in the presence of optical distortions. In robotics, this ability is referred to as visual servoing: moving a tool or end-point to a desired location using primarily visual feedback. In this paper, we study how viewpoint-invariant visual servoing skills can be learned automatically in a robotic manipulation scenario. To this end, we train a deep recurrent controller that can automatically determine which actions move the end-point of a robotic arm to a desired object. The problem that must be solved by this controller is fundamentally ambiguous: under severe variation in viewpoint, it may be impossible to determine the actions in a single feedforward operation. Instead, our visual servoing system must use its memory of past movements to understand how the actions affect the robot motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This ability is in stark contrast to most visual servoing methods, which either assume known dynamics or require a calibration phase. We show how we can learn this recurrent controller using simulated data and a reinforcement learning objective. We then describe how the resulting model can be transferred to a real-world robot by disentangling perception from control and only adapting the visual layers. The adapted model can servo to previously unseen objects from novel viewpoints on a real-world Kuka IIWA robotic arm. For supplementary videos, see: https://fsadeghi.github.io/Sim2RealViewInvariantServo},
archivePrefix = {arXiv},
arxivId = {1712.07642},
author = {Sadeghi, Fereshteh and Toshev, Alexander and Jang, Eric and Levine, Sergey},
eprint = {1712.07642},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sadeghi et al. - 2017 - Sim2Real View Invariant Visual Servoing by Recurrent Control.pdf:pdf},
month = {dec},
title = {{Sim2Real View Invariant Visual Servoing by Recurrent Control}},
url = {http://arxiv.org/abs/1712.07642},
year = {2017}
}
@inproceedings{Zhu2017,
abstract = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new target goals, and (2) data inefficiency i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to the task of target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows to better generalize. To address the second issue, we propose AI2-THOR framework, which provides an environment with high-quality 3D scenes and physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and across scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment. The supplementary video can be accessed at the following link: https://youtu.be/SmBxMDiOrvs.},
address = {Singapore, Singapore},
archivePrefix = {arXiv},
arxivId = {1609.05143},
author = {Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J. and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
eprint = {1609.05143},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2017 - Target-driven visual navigation in indoor scenes using deep reinforcement learning.pdf:pdf},
isbn = {978-1-5090-4633-1},
month = {may},
pages = {3357--3364},
publisher = {IEEE},
title = {{Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning}},
url = {http://ieeexplore.ieee.org/document/7989381/},
year = {2017}
}
@inproceedings{Li2016,
abstract = {Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted , predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chat-bot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: infor-mativity, coherence, and ease of answering (re-lated to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.},
address = {Austin, Texas},
author = {Li, Jiwei and Monroe, Will and Ritter, Alan and Galley, Michel and Gao, Jianfeng and Jurafsky, Dan},
booktitle = {2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/D16-1127},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - Deep Reinforcement Learning for Dialogue Generation.pdf:pdf},
pages = {1192--1202},
publisher = {Association for Computational Linguistics},
title = {{Deep Reinforcement Learning for Dialogue Generation}},
url = {https://en.wikipedia.org/wiki/Yes,{\_}and...},
volume = {abs/1606.0},
year = {2016}
}
@inproceedings{Mao2016,
address = {New York, New York, USA},
author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks - HotNets '16},
doi = {10.1145/3005745.3005750},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2016 - Resource Management with Deep Reinforcement Learning.pdf:pdf},
isbn = {9781450346610},
pages = {50--56},
publisher = {ACM Press},
title = {{Resource Management with Deep Reinforcement Learning}},
url = {http://dl.acm.org/citation.cfm?doid=3005745.3005750},
year = {2016}
}
@incollection{NIPS1994_1018,
author = {Boyan, Justin A and Moore, Andrew W},
booktitle = {Advances in Neural Information Processing Systems 7},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyan, Moore - 1995 - Generalization in Reinforcement Learning Safely Approximating the Value Function.pdf:pdf},
pages = {369--376},
publisher = {MIT Press},
title = {{Generalization in Reinforcement Learning: Safely Approximating the Value Function}},
url = {http://papers.nips.cc/paper/1018-generalization-in-reinforcement-learning-safely-approximating-the-value-function.pdf},
year = {1995}
}
@inproceedings{Lange2012,
abstract = {We propose a learning architecture, that is able to do reinforcement learning based on raw visual input data. In contrast to previous approaches, not only the control policy is learned. In order to be successful, the system must also autonomously learn, how to extract relevant information out of a high-dimensional stream of input information, for which the semantics are not provided to the learning system. We give a first proof-of-concept of this novel learning architecture on a challenging benchmark, namely visual control of a racing slot car. The resulting policy, learned only by success or failure, is hardly beaten by an experienced human player},
author = {Lange, Sascha and Riedmiller, Martin and Voigtlander, Arne},
booktitle = {The 2012 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2012.6252823},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lange, Riedmiller, Voigtlander - 2012 - Autonomous reinforcement learning on raw visual input data in a real world application.pdf:pdf},
isbn = {978-1-4673-1490-9},
month = {jun},
pages = {1--8},
publisher = {IEEE},
title = {{Autonomous reinforcement learning on raw visual input data in a real world application}},
url = {http://ieeexplore.ieee.org/document/6252823/},
year = {2012}
}
@inproceedings{Mnih2014,
abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1406.6247},
author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
booktitle = {NIPS'14 Proceedings of the 27th International Conference on Neural Information Processing Systems},
eprint = {1406.6247},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2014 - Recurrent Models of Visual Attention.pdf:pdf},
month = {jun},
pages = {2204--2212 },
publisher = {MIT Press},
title = {{Recurrent Models of Visual Attention}},
url = {https://arxiv.org/abs/1406.6247},
year = {2014}
}
@inproceedings{Finn2016,
abstract = {A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation -- pushing objects -- and can handle novel objects not seen during training.},
archivePrefix = {arXiv},
arxivId = {1610.00696},
author = {Finn, Chelsea and Levine, Sergey},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
eprint = {1610.00696},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Finn, Levine - 2016 - Deep Visual Foresight for Planning Robot Motion.pdf:pdf},
month = {oct},
pages = {2786--2793},
title = {{Deep Visual Foresight for Planning Robot Motion}},
url = {http://arxiv.org/abs/1610.00696},
year = {2016}
}
@article{Peters2008,
abstract = {Autonomous learning is one of the hallmarks of human and animal behavior, and understanding the principles of learning will be crucial in order to achieve true autonomy in advanced machines like humanoid robots. In this paper, we examine learning of complex motor skills with human-like limbs. While supervised learning can offer useful tools for bootstrapping behavior, e.g., by learning from demonstration, it is only reinforcement learning that offers a general approach to the final trial-and-error improvement that is needed by each individual acquiring a skill. Neither neurobiological nor machine learning studies have, so far, offered compelling results on how reinforcement learning can be scaled to the high-dimensional continuous state and action spaces of humans or humanoids. Here, we combine two recent research developments on learning motor control in order to achieve this scaling. First, we interpret the idea of modular motor control by means of motor primitives as a suitable way to generate parameterized control policies for reinforcement learning. Second, we combine motor primitives with the theory of stochastic policy gradient learning, which currently seems to be the only feasible framework for reinforcement learning for humanoids. We evaluate different policy gradient methods with a focus on their applicability to parameterized motor primitives. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm.},
author = {Peters, Jan and Schaal, Stefan},
doi = {10.1016/j.neunet.2008.02.003},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peters, Schaal - 2008 - Reinforcement learning of motor skills with policy gradients.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
month = {may},
number = {4},
pages = {682--697},
pmid = {18482830},
title = {{Reinforcement learning of motor skills with policy gradients}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18482830 http://linkinghub.elsevier.com/retrieve/pii/S0893608008000701},
volume = {21},
year = {2008}
}
@inproceedings{Dhingra2017,
abstract = {This paper proposes KB-InfoBot -- a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced "soft" posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents. The source code is available at https://github.com/MiuLab/KB-InfoBot.},
address = {Vancouver, Canada },
archivePrefix = {arXiv},
arxivId = {1609.00777},
author = {Dhingra, Bhuwan and Li, Lihong and Li, Xiujun and Gao, Jianfeng and Chen, Yun-Nung and Ahmed, Faisal and Deng, Li},
booktitle = { 55th Annual Meeting of the Association for Computational Linguistics},
eprint = {1609.00777},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dhingra et al. - 2017 - Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access.pdf:pdf},
month = {jul},
pages = {484--495 },
publisher = {Association for Computational Linguistics },
title = {{Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access}},
url = {http://arxiv.org/abs/1609.00777},
year = {2017}
}
@inproceedings{Barth-Maron2018,
abstract = {This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of {\$}N{\$}-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.},
archivePrefix = {arXiv},
arxivId = {1804.08617},
author = {Barth-Maron, Gabriel and Hoffman, Matthew W. and Budden, David and Dabney, Will and Horgan, Dan and TB, Dhruva and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
booktitle = {International Conference on Learning Representations},
eprint = {1804.08617},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barth-Maron et al. - 2018 - Distributed Distributional Deterministic Policy Gradients.pdf:pdf},
month = {apr},
title = {{Distributed Distributional Deterministic Policy Gradients}},
url = {http://arxiv.org/abs/1804.08617},
year = {2018}
}
@inproceedings{Narasimhan2015,
abstract = {In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.},
address = {Lisbon, Portugal},
archivePrefix = {arXiv},
arxivId = {1506.08941},
author = {Narasimhan, Karthik and Kulkarni, Tejas and Barzilay, Regina},
booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP) 2015},
eprint = {1506.08941},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Narasimhan, Kulkarni, Barzilay - 2015 - Language Understanding for Text-based Games Using Deep Reinforcement Learning.pdf:pdf},
month = {jun},
title = {{Language Understanding for Text-based Games Using Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1506.08941},
year = {2015}
}
@book{Bellman:DynamicProgramming,
abstract = {An introduction to the mathematical theory of multistage decision processes, this text takes a "functional equation" approach to the discovery of optimum policies. Written by a leading developer of such policies, it presents a series of methods, uniqueness and existence theorems, and examples for solving the relevant equations. The text examines existence and uniqueness theorems, the optimal inventory equation, bottleneck problems in multistage production processes, a new formalism in the calculus of variation, strategies behind multistage games, and Markovian decision processes. Each chapter concludes with a problem set that Eric V. Denardo of Yale University, in his informative new introduction, calls "a rich lode of applications and research topics." 1957 edition. 37 figures.},
author = {Bellman, Richard},
isbn = {9780486428093},
keywords = {book dynamic programming},
publisher = {Dover Publications},
title = {{Dynamic Programming}},
year = {1957}
}
@inproceedings{Hagan1999,
abstract = {Provides a quick overview of neural networks and explains how they can be used in control systems. We introduce the multilayer perceptron neural network and describe how it can be used for function approximation. The backpropagation algorithm (including its variations) is the principal procedure for training multilayer perceptrons; it is briefly described here. Care must be taken, when training perceptron networks, to ensure that they do not overfit the training data and then fail to generalize well in new situations. Several techniques for improving generalization are discussed. The article also presents several control architectures, such as model reference adaptive control, model predictive control, and internal model control, in which multilayer perceptron neural networks can be used as basic building blocks.},
address = {San Diego, CA, USA},
author = {Hagan, M.T. and Demuth, H.B.},
booktitle = {Proceedings of the 1999 American Control Conference (Cat. No. 99CH36251)},
doi = {10.1109/ACC.1999.786109},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hagan, Demuth - 1999 - Neural networks for control.pdf:pdf},
isbn = {0-7803-4990-3},
pages = {1642--1656 vol.3},
publisher = {IEEE},
title = {{Neural networks for control}},
url = {http://ieeexplore.ieee.org/document/786109/},
year = {1999}
}
@inproceedings{Finn2016a,
abstract = {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1603.00448},
author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
booktitle = {33rd International Conference on Machine Learning (ICML) - Volume 48},
eprint = {1603.00448},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Finn, Levine, Abbeel - 2016 - Guided Cost Learning Deep Inverse Optimal Control via Policy Optimization.pdf:pdf},
month = {mar},
pages = {49--58},
publisher = {JMLR.org},
title = {{Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}},
url = {http://arxiv.org/abs/1603.00448},
year = {2016}
}
@inproceedings{Finn2017,
abstract = {In order for a robot to be a generalist that can perform a wide range of jobs, it must be able to acquire a wide variety of skills quickly and efficiently in complex unstructured environments. High-capacity models such as deep neural networks can enable a robot to represent complex skills, but learning each skill from scratch then becomes infeasible. In this work, we present a meta-imitation learning method that enables a robot to learn how to learn more efficiently, allowing it to acquire new skills from just a single demonstration. Unlike prior methods for one-shot imitation, our method can scale to raw pixel inputs and requires data from significantly fewer prior tasks for effective learning of new skills. Our experiments on both simulated and real robot platforms demonstrate the ability to learn new tasks, end-to-end, from a single visual demonstration.},
address = {Mountain View, California},
archivePrefix = {arXiv},
arxivId = {1709.04905},
author = {Finn, Chelsea and Yu, Tianhe and Zhang, Tianhao and Abbeel, Pieter and Levine, Sergey},
booktitle = {Conference on Robot Learning (CoRL)},
eprint = {1709.04905},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Finn et al. - 2017 - One-Shot Visual Imitation Learning via Meta-Learning.pdf:pdf},
month = {sep},
title = {{One-Shot Visual Imitation Learning via Meta-Learning}},
url = {http://arxiv.org/abs/1709.04905},
year = {2017}
}
@inproceedings{Christiano2017,
author = {Christiano, Paul F. and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christiano et al. - 2017 - Deep Reinforcement Learning from Human Preferences.pdf:pdf},
pages = {4299--4307},
title = {{Deep Reinforcement Learning from Human Preferences}},
url = {http://papers.nips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences},
year = {2017}
}
@article{Heess2017,
abstract = {The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx{\_}bgoTF7bs .},
archivePrefix = {arXiv},
arxivId = {1707.02286},
author = {Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S. M. Ali and Riedmiller, Martin and Silver, David},
eprint = {1707.02286},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heess et al. - 2017 - Emergence of Locomotion Behaviours in Rich Environments.pdf:pdf},
month = {jul},
title = {{Emergence of Locomotion Behaviours in Rich Environments}},
url = {http://arxiv.org/abs/1707.02286},
year = {2017}
}
@article{Narendra1990,
author = {Narendra, K.S. and Parthasarathy, K.},
doi = {10.1109/72.80202},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Narendra, Parthasarathy - 1990 - Identification and control of dynamical systems using neural networks.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
month = {mar},
number = {1},
pages = {4--27},
title = {{Identification and control of dynamical systems using neural networks}},
url = {http://ieeexplore.ieee.org/document/80202/},
volume = {1},
year = {1990}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 2015 - Deep Learning in neural networks An overview(2).pdf:pdf},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
pmid = {25462637},
publisher = {Elsevier Ltd},
title = {{Deep Learning in neural networks: An overview}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.09.003},
volume = {61},
year = {2015}
}
@article{Argall2009,
abstract = {We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research.},
author = {Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
doi = {10.1016/j.robot.2008.10.024},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Argall et al. - 2009 - A survey of robot learning from demonstration.pdf:pdf},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous systems,Learning from demonstration,Machine learning,Robotics},
pages = {469--483},
title = {{A survey of robot learning from demonstration}},
url = {www.elsevier.com/locate/robot},
volume = {57},
year = {2009}
}
@misc{OpenAI2017,
author = {OpenAI},
title = {{OpenAI Dota 2 1v1 bot}},
url = {https://openai.com/the-international/},
urldate = {2018-11-24},
year = {2017}
}
@inproceedings{Gu2017a,
abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
archivePrefix = {arXiv},
arxivId = {1611.02247},
author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
booktitle = {Interantional Conference for Learning Representations (ICLR)},
eprint = {1611.02247},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2017 - Q-Prop Sample-Efficient Policy Gradient with An Off-Policy Critic.pdf:pdf},
month = {nov},
title = {{Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic}},
url = {http://arxiv.org/abs/1611.02247},
year = {2017}
}
@article{Nair2015,
abstract = {We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.},
archivePrefix = {arXiv},
arxivId = {1507.04296},
author = {Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and {De Maria}, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and Legg, Shane and Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David},
eprint = {1507.04296},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nair et al. - 2015 - Massively Parallel Methods for Deep Reinforcement Learning(2).pdf:pdf},
month = {jul},
title = {{Massively Parallel Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1507.04296},
year = {2015}
}
@article{Haarnoja2018a,
abstract = {Deep reinforcement learning suggests the promise of fully automated learning of robotic control policies that directly map sensory inputs to low-level actions. However, applying deep reinforcement learning methods on real-world robots is exceptionally difficult, due both to the sample complexity and, just as importantly, the sensitivity of such methods to hyperparameters. While hyperparameter tuning can be performed in parallel in simulated domains, it is usually impractical to tune hyperparameters directly on real-world robotic platforms, especially legged platforms like quadrupedal robots that can be damaged through extensive trial-and-error learning. In this paper, we develop a stable variant of the soft actor-critic deep reinforcement learning algorithm that requires minimal hyperparameter tuning, while also requiring only a modest number of trials to learn multilayer neural network policies. This algorithm is based on the framework of maximum entropy reinforcement learning, and automatically trades off exploration against exploitation by dynamically and automatically tuning a temperature parameter that determines the stochasticity of the policy. We show that this method achieves state-of-the-art performance on four standard benchmark environments. We then demonstrate that it can be used to learn quadrupedal locomotion gaits on a real-world Minitaur robot, learning to walk from scratch directly in the real world in two hours of training.},
archivePrefix = {arXiv},
arxivId = {1812.11103},
author = {Haarnoja, Tuomas and Zhou, Aurick and Ha, Sehoon and Tan, Jie and Tucker, George and Levine, Sergey},
eprint = {1812.11103},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haarnoja et al. - 2018 - Learning to Walk via Deep Reinforcement Learning.pdf:pdf},
month = {dec},
title = {{Learning to Walk via Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1812.11103},
year = {2018}
}
@incollection{VanHasselt2010,
author = {van Hasselt, Hado},
booktitle = {Advances in Neural Information Processing Systems 23},
pages = {2613--2621},
publisher = {Curran Associates, Inc.},
title = {{Double Q-learning}},
url = {http://papers.nips.cc/paper/3964-double-q-learning.pdf},
year = {2010}
}
@inproceedings{Pollack1997a,
author = {Pollack, Jordan B and Blair, Alan D},
booktitle = {Advances in Neural Information Processing Systems},
pages = {10--16},
title = {{Why did TD-gammon work?}},
year = {1997}
}
@article{Fridman2018,
abstract = {We present a micro-traffic simulation (named "DeepTraffic") where the perception, control, and planning systems for one of the cars are all handled by a single neural network as part of a model-free, off-policy reinforcement learning process. The primary goal of DeepTraffic is to make the hands-on study of deep reinforcement learning accessible to thousands of students, educators, and researchers in order to inspire and fuel the exploration and evaluation of DQN variants and hyperparameter configurations through large-scale, open competition. This paper investigates the crowd-sourced hyperparameter tuning of the policy network that resulted from the first iteration of the DeepTraffic competition where thousands of participants actively searched through the hyperparameter space with the objective of their neural network submission to make it onto the top-10 leaderboard.},
archivePrefix = {arXiv},
arxivId = {1801.02805},
author = {Fridman, Lex and Jenik, Benedikt and Terwilliger, Jack},
eprint = {1801.02805},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fridman, Jenik, Terwilliger - 2018 - DeepTraffic Driving Fast through Dense Traffic with Deep Reinforcement Learning.pdf:pdf},
month = {jan},
title = {{DeepTraffic: Driving Fast through Dense Traffic with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1801.02805},
year = {2018}
}
@inproceedings{Bansal2017a,
abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
archivePrefix = {arXiv},
arxivId = {1710.03748},
author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1710.03748},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bansal et al. - 2017 - Emergent Complexity via Multi-Agent Competition(2).pdf:pdf},
month = {oct},
title = {{Emergent Complexity via Multi-Agent Competition}},
url = {http://arxiv.org/abs/1710.03748},
year = {2017}
}
@article{Rusu2016a,
abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
archivePrefix = {arXiv},
arxivId = {1606.04671},
author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
eprint = {1606.04671},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rusu et al. - 2016 - Progressive Neural Networks.pdf:pdf},
month = {jun},
title = {{Progressive Neural Networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
@phdthesis{Watkins1989,
author = {Watkins, Christopher John Cornish Hellaby},
school = {King's College, Cambridge},
title = {{Learning from delayed rewards}},
year = {1989}
}
@article{Moravcik2017,
abstract = {Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold'em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.},
author = {Morav{\v{c}}{\'{i}}k, Matej and Schmid, Martin and Burch, Neil and Lis{\'{y}}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
doi = {10.1126/science.aam6960},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morav{\v{c}}{\'{i}}k et al. - 2017 - DeepStack Expert-level artificial intelligence in heads-up no-limit poker.pdf:pdf},
issn = {1095-9203},
journal = {Science},
month = {may},
number = {6337},
pages = {508--513},
pmid = {28254783},
publisher = {American Association for the Advancement of Science},
title = {{DeepStack: Expert-level artificial intelligence in heads-up no-limit poker.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28254783},
volume = {356},
year = {2017}
}
@inproceedings{Bellemare2017,
abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
annote = {D4PG basic paper},
archivePrefix = {arXiv},
arxivId = {1707.06887},
author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'{e}}mi},
booktitle = {International Conference for Machine Learning (ICML)},
eprint = {1707.06887},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellemare, Dabney, Munos - 2017 - A Distributional Perspective on Reinforcement Learning.pdf:pdf},
month = {jul},
title = {{A Distributional Perspective on Reinforcement Learning}},
url = {http://arxiv.org/abs/1707.06887},
year = {2017}
}
@inproceedings{Kakade2001,
address = {Cambridge, MA, USA},
author = {Kakade, Sham},
booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
pages = {1531--1538},
publisher = {MIT Press},
series = {NIPS'01},
title = {{A Natural Policy Gradient}},
url = {http://dl.acm.org/citation.cfm?id=2980539.2980738},
year = {2001}
}
@inproceedings{Chen2018,
abstract = {Deep reinforcement learning could be used to learn dexterous robotic policies but it is challenging to transfer them to new robots with vastly different hardware properties. It is also prohibitively expensive to learn a new policy from scratch for each robot hardware due to the high sample complexity of modern state-of-the-art algorithms. We propose a novel approach called Hardware Conditioned Policies where we train a universal policy conditioned on a vector representation of robot hardware. We considered robots in simulation with varied dynamics, kinematic structure, kinematic lengths and degrees-of-freedom. First, we use the kinematic structure directly as the hardware encoding and show great zero-shot transfer to completely novel robots not seen during training. For robots with lower zero-shot success rate, we also demonstrate that fine-tuning the policy network is significantly more sample-efficient than training a model from scratch. In tasks where knowing the agent dynamics is important for success, we learn an embedding for robot hardware and show that policies conditioned on the encoding of hardware tend to generalize and transfer well. Videos of experiments are available at: https://sites.google.com/view/robot-transfer-hcp.},
address = {Montreal, Canada},
author = {Chen, Tao and Murali, Adithyavairavan and Gupta, Abhinav},
booktitle = {Conference on Neural Information Processing Systems (NIPS) 32},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Murali, Gupta - 2018 - Hardware Conditioned Policies for Multi-Robot Transfer Learning.pdf:pdf},
title = {{Hardware Conditioned Policies for Multi-Robot Transfer Learning}},
url = {https://sites.google.com/view/robot-transfer-hcp.},
year = {2018}
}
@article{Kaelbling1996,
abstract = {{\textless}p{\textgreater}This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.{\textless}/p{\textgreater}},
author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
doi = {10.1613/jair.301},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaelbling, Littman, Moore - 1996 - Reinforcement Learning A Survey.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
month = {may},
pages = {237--285},
title = {{Reinforcement Learning: A Survey}},
url = {https://jair.org/index.php/jair/article/view/10166},
volume = {4},
year = {1996}
}
@article{Bellemare2013,
abstract = {{\textless}p{\textgreater}In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.{\textless}/p{\textgreater}},
archivePrefix = {arXiv},
arxivId = {1207.4708},
author = {Bellemare, M. G. and Naddaf, Y. and Veness, J. and Bowling, M.},
doi = {10.1613/jair.3912},
eprint = {1207.4708},
journal = {Journal of Artificial Intelligence Research},
month = {jun},
pages = {253--279},
title = {{The Arcade Learning Environment: An Evaluation Platform for General Agents}},
url = {http://arxiv.org/abs/1207.4708},
volume = {47},
year = {2013}
}
@inproceedings{Watter2015,
abstract = {We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.},
archivePrefix = {arXiv},
arxivId = {1506.07365},
author = {Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
booktitle = {Neural Information Processing Systems (NIPS)},
eprint = {1506.07365},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watter et al. - 2015 - Embed to Control A Locally Linear Latent Dynamics Model for Control from Raw Images.pdf:pdf},
month = {jun},
title = {{Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images}},
url = {http://arxiv.org/abs/1506.07365},
year = {2015}
}
@inproceedings{7989383,
abstract = {Autonomous learning of robotic skills can allow general-purpose robots to learn wide behavioral repertoires without extensive manual engineering. However, robotic skill learning must typically make trade-offs to enable practical real-world learning, such as requiring manually designed policy or value function representations, initialization from human demonstrations, instrumentation of the training environment, or extremely long training times. We propose a new reinforcement learning algorithm that can train general-purpose neural network policies with minimal human engineering, while still allowing for fast, efficient learning in stochastic environments. We build on the guided policy search (GPS) algorithm, which transforms the reinforcement learning problem into supervised learning from a computational teacher (without human demonstrations). In contrast to prior GPS methods, which require a consistent set of initial states to which the system must be reset after each episode, our approach can handle random initial states, allowing it to be used even when deterministic resets are impossible. We compare our method to existing policy search algorithms in simulation, showing that it can train high-dimensional neural network policies with the same sample efficiency as prior GPS methods, and can learn policies directly from image pixels. We also present real-world robot results that show that our method can learn manipulation policies with visual features and random initial states.},
annote = {basic for MAP},
author = {Montgomery, W and Ajay, A and Finn, C and Abbeel, P and Levine, S},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989383},
file = {:home/fabian/Desktop/Reset-free guided policy search$\backslash$: Efficient deep reinforcement learning with stochastic initial states.pdf:pdf},
keywords = {GPS algorithm,Global Positioning System,Learning (artificial intelligence),Neural networks,Optimization,Robots,Supervised learning,Training,autonomous learning,deep reinforcement learning,general-purpose neural network policies,high-dimensional neural network policies,image pixels,learning (artificial intelligence),neurocontrollers,reset-free guided policy search,robotic skill learning,robots,stochastic initial states},
month = {may},
pages = {3373--3380},
title = {{Reset-free guided policy search: Efficient deep reinforcement learning with stochastic initial states}},
year = {2017}
}
@inproceedings{Boyan1993,
abstract = {This paper describes the Q-routing algorithm for packet routing, in which a reinforcement learning module is embedded into each node of a switching network. Only local communication is used by each node to keep accurate statistics on which routing decisions lead to minimal delivery times. In simple experiments involving a 36-node, irregularly connected network, Q-routing proves superior to a nonadaptive algorithm based on precomputed shortest paths and is able to route efficiently even when critical aspects of the simulation, such as the network load, are allowed to vary dynamically. The paper concludes with a discussion of the tradeoff between discovering shortcuts and maintaining stable policies.},
address = {Denver, Colorado},
author = {Boyan, Justin A. and Littman, Michael L.},
booktitle = {Proceedings of the 6th International Conference on Neural Information Processing Systems},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyan, Littman - 1993 - Packet routing in dynamically changing networks a reinforcement learning approach.pdf:pdf},
pages = {671--678},
publisher = {Morgan Kaufmann},
title = {{Packet routing in dynamically changing networks: a reinforcement learning approach}},
url = {https://dl.acm.org/citation.cfm?id=2987274},
year = {1993}
}
@article{Justesen2017,
abstract = {In this article, we review recent Deep Learning advances in the context of how they have been applied to play different types of video games such as first-person shooters, arcade games, and real-time strategy games. We analyze the unique requirements that different game genres pose to a deep learning system and highlight important open challenges in the context of applying these machine learning methods to video games, such as general game playing, dealing with extremely large decision spaces and sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1708.07902},
author = {Justesen, Niels and Bontrager, Philip and Togelius, Julian and Risi, Sebastian},
eprint = {1708.07902},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Justesen et al. - 2017 - Deep Learning for Video Game Playing.pdf:pdf},
month = {aug},
title = {{Deep Learning for Video Game Playing}},
url = {http://arxiv.org/abs/1708.07902},
year = {2017}
}
@article{Sun2018,
abstract = {Starcraft II (SC2) is widely considered as the most challenging Real Time Strategy (RTS) game. The underlying challenges include a large observation space, a huge (continuous and infinite) action space, partial observations, simultaneous move for all players, and long horizon delayed rewards for local decisions. To push the frontier of AI research, Deepmind and Blizzard jointly developed the StarCraft II Learning Environment (SC2LE) as a testbench of complex decision making systems. SC2LE provides a few mini games such as MoveToBeacon, CollectMineralShards, and DefeatRoaches, where some AI agents have achieved the performance level of human professional players. However, for full games, the current AI agents are still far from achieving human professional level performance. To bridge this gap, we present two full game AI agents in this paper - the AI agent TStarBot1 is based on deep reinforcement learning over a flat action structure, and the AI agent TStarBot2 is based on hard-coded rules over a hierarchical action structure. Both TStarBot1 and TStarBot2 are able to defeat the built-in AI agents from level 1 to level 10 in a full game (1v1 Zerg-vs-Zerg game on the AbyssalReef map), noting that level 8, level 9, and level 10 are cheating agents with unfair advantages such as full vision on the whole map and resource harvest boosting. To the best of our knowledge, this is the first public work to investigate AI agents that can defeat the built-in AI in the StarCraft II full game.},
archivePrefix = {arXiv},
arxivId = {1809.07193},
author = {Sun, Peng and Sun, Xinghai and Han, Lei and Xiong, Jiechao and Wang, Qing and Li, Bo and Zheng, Yang and Liu, Ji and Liu, Yongsheng and Liu, Han and Zhang, Tong},
eprint = {1809.07193},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - 2018 - TStarBots Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game.pdf:pdf},
month = {sep},
title = {{TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game}},
url = {http://arxiv.org/abs/1809.07193},
year = {2018}
}
@article{Zhang2015,
abstract = {This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.},
archivePrefix = {arXiv},
arxivId = {1511.03791},
author = {Zhang, Fangyi and Leitner, J{\"{u}}rgen and Milford, Michael and Upcroft, Ben and Corke, Peter},
eprint = {1511.03791},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2015 - Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control.pdf:pdf},
journal = {Australasian Conference on Robotics and Automation (ACRA) 2015},
month = {nov},
title = {{Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control}},
url = {http://arxiv.org/abs/1511.03791},
year = {2015}
}
@article{Bakker2003,
abstract = {It is difficult to apply traditional reinforcement learning algorithms to robots, due to problems with large and continuous domains, partial observability, and limited numbers of learning experiences. This paper deals with these problems by combining: (1) reinforcement learning with memory, implemented using an LSTM recurrent neural network whose inputs are discrete events extracted from raw inputs; (2) online exploration and offline policy learning. An experiment with a real robot demonstrates the methodology's feasibility.},
author = {Bakker, Bram and Zhumatiy, V and Gruener, G and Schmidhuber, J{\"{u}}rgen},
doi = {10.1109/IROS.2003.1250667},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakker et al. - 2003 - A robot that reinforcement-learns to identify and memorize important previous observations.pdf:pdf},
isbn = {0-7803-7860-1},
journal = {Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)},
pages = {430--435},
pmid = {11465086},
title = {{A robot that reinforcement-learns to identify and memorize important previous observations}},
volume = {1},
year = {2003}
}
@article{Choi2015,
abstract = {The automatic, non-line-of-sight characteristics of radio frequency identification (RFID) technology for identifying multiple objects are conducive to full visibility and traceability of individual product items in a supply chain. However, practical implementation of item-level RFID-based applications necessitates solving some critical issues. Among these issues, reading tag data to identify a relatively large number of individual product items, which are usually packed in batches and distributed on pallets, is particularly a bottleneck, because it affects the accuracy and trustworthiness of batch distribution of products and all subsequent logistics operations in the supply chain. Current techniques for batch identification at item-level of palletised products suffer low reading rate and incomplete tag data acquisition, rendering the RFID systems unreliable. We address this issue by proposing a mechanised 3D scanning method for identification of tagged products in large numbers to facilitate supply chain management. The proposed method requires installing RFID readers only in the X-Y plane. The readers scan tagged products in the X and Y directions while the pallet is simultaneously rotated around to be effectively scanned in the Z direction. Different scanning patterns are adopted to alleviate the problems due to randomness of tag orientation and reader collisions. As such, 3D scanning of RFID tags for item-level applications is effectively achieved without incurring much hardware cost. The performance of the proposed method is validated using an RFID-enabled gate-door for identification of palletised apparel products with item-level RFID tagging. Experiment results show that the proposed method can achieve batch reading rates remarkably higher than those reported in literature. Moreover, a correlation between the batch reading rate and the batch density is established. Apparently, the proposed mechanised 3D scanning method for batch identification of item-level tagged product items can substantially enhance the accuracy and reliability of RFID-based supply chain management systems.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Choi, S. H. and Yang, B. and Cheung, H. H.},
doi = {10.1016/j.compind.2015.04.001},
eprint = {1312.5602},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Yang, Cheung - 2015 - A mechanised 3D scanning method for item-level radio frequency identification of palletised products.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {01663615},
journal = {Computers in Industry},
keywords = {Batch reading,Item-level RFID,Mechanised 3D scanning,Product identification},
month = {dec},
pages = {36--46},
pmid = {25719670},
title = {{A mechanised 3D scanning method for item-level radio frequency identification of palletised products}},
url = {http://arxiv.org/abs/1312.5602},
volume = {72},
year = {2015}
}
@inproceedings{Peng2017,
abstract = {Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this "reality gap". By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.},
address = {Montreal, Canada},
archivePrefix = {arXiv},
arxivId = {1710.06537},
author = {Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
booktitle = {Conference on Neural Information Processing Systems (NIPS)},
doi = {10.1109/ICRA.2018.8460528},
eprint = {1710.06537},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng et al. - 2017 - Sim-to-Real Transfer of Robotic Control with Dynamics Randomization.pdf:pdf},
month = {oct},
title = {{Sim-to-Real Transfer of Robotic Control with Dynamics Randomization}},
url = {http://arxiv.org/abs/1710.06537 http://dx.doi.org/10.1109/ICRA.2018.8460528},
year = {2017}
}
@inproceedings{Cuccu2011,
abstract = {Neuroevolution, the artificial evolution of neural networks, has shown great promise on continuous reinforcement learning tasks that require memory. However, it is not yet directly applicable to realistic embedded agents using high-dimensional (e.g. raw video images) inputs, requiring very large networks. In this paper, neuroevolution is combined with an unsupervised sensory pre-processor or compressor that is trained on images generated from the environment by the population of evolving recurrent neural network controllers. The compressor not only reduces the input cardinality of the controllers, but also biases the search toward novel controllers by rewarding those controllers that discover images that it reconstructs poorly. The method is successfully demonstrated on a vision-based version of the well-known mountain car benchmark, where controllers receive only single high-dimensional visual images of the environment, from a third-person perspective, instead of the standard two-dimensional state vector which includes information about velocity.},
annote = {Applies NFQ (Riedmiller, 2005)},
author = {Cuccu, Giuseppe and Luciw, Matthew and Schmidhuber, Jurgen and Gomez, Faustino},
booktitle = {2011 IEEE International Conference on Development and Learning (ICDL)},
doi = {10.1109/DEVLRN.2011.6037324},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cuccu et al. - 2011 - Intrinsically motivated neuroevolution for vision-based reinforcement learning.pdf:pdf},
isbn = {978-1-61284-989-8},
month = {aug},
pages = {1--7},
publisher = {IEEE},
title = {{Intrinsically motivated neuroevolution for vision-based reinforcement learning}},
url = {http://ieeexplore.ieee.org/document/6037324/},
year = {2011}
}
@article{Peters2008a,
author = {Peters, Jan and Schaal, Stefan},
journal = {Neurocomputing},
number = {7-9},
pages = {1180--1190},
publisher = {Elsevier},
title = {{Natural actor-critic}},
volume = {71},
year = {2008}
}
@article{Popov2017,
abstract = {Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.},
archivePrefix = {arXiv},
arxivId = {1704.03073},
author = {Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin},
eprint = {1704.03073},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Popov et al. - 2017 - Data-efficient Deep Reinforcement Learning for Dexterous Manipulation.pdf:pdf},
month = {apr},
title = {{Data-efficient Deep Reinforcement Learning for Dexterous Manipulation}},
url = {http://arxiv.org/abs/1704.03073},
year = {2017}
}
@incollection{Kulkarni2016,
author = {Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
booktitle = {Advances in Neural Information Processing Systems 29},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni et al. - 2016 - Hierarchical Deep Reinforcement Learning Integrating Temporal Abstraction and Intrinsic Motivation.pdf:pdf},
pages = {3675--3683},
publisher = {Curran Associates, Inc.},
title = {{Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation}},
url = {http://papers.nips.cc/paper/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation.pdf},
year = {2016}
}
@inproceedings{Wang2016,
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {Van Hasselt}, Hado and Lanctot, Marc and {De Freitas}, Nando},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
file = {:home/fabian/Desktop/Dueling Network Architectures for Deep Reinforcement Learning.pdf:pdf},
pages = {1995--2003},
publisher = {JMLR.org},
series = {ICML'16},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
url = {http://dl.acm.org/citation.cfm?id=3045390.3045601},
year = {2016}
}
@inproceedings{Grosse2016,
author = {Grosse, Roger and Martens, James},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {573--582},
publisher = {JMLR.org},
series = {ICML'16},
title = {{A Kronecker-factored Approximate Fisher Matrix for Convolution Layers}},
url = {http://dl.acm.org/citation.cfm?id=3045390.3045452},
year = {2016}
}
@book{Sutton2018,
address = {Cambridge, MA},
author = {Sutton, Richard S and Barto, Andrew G},
edition = {Second Edi},
publisher = {MIT Press},
title = {{Reinforcement learning: An introduction}},
url = {http://incompleteideas.net/book/the-book.html},
year = {2018}
}
@article{Yu2018,
abstract = {We consider the problem of learning multi-stage vision-based tasks on a real robot from a single video of a human performing the task, while leveraging demonstration data of subtasks with other objects. This problem presents a number of major challenges. Video demonstrations without teleoperation are easy for humans to provide, but do not provide any direct supervision. Learning policies from raw pixels enables full generality but calls for large function approximators with many parameters to be learned. Finally, compound tasks can require impractical amounts of demonstration data, when treated as a monolithic skill. To address these challenges, we propose a method that learns both how to learn primitive behaviors from video demonstrations and how to dynamically compose these behaviors to perform multi-stage tasks by "watching" a human demonstrator. Our results on a simulated Sawyer robot and real PR2 robot illustrate our method for learning a variety of order fulfillment and kitchen serving tasks with novel objects and raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1810.11043},
author = {Yu, Tianhe and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
eprint = {1810.11043},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu et al. - 2018 - One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks.pdf:pdf},
month = {oct},
title = {{One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks}},
url = {http://arxiv.org/abs/1810.11043},
year = {2018}
}
@inproceedings{Sutton1990,
author = {Sutton, Richard S},
booktitle = {International Conference for Machine Learning (ICML)2},
pages = {216--224},
title = {{Integrated architectures for learning, planning, reacting based on approxmiate dynmaic programming}},
year = {1990}
}
@inproceedings{McAleer2018,
abstract = {A generally intelligent agent must be able to teach itself how to solve problems in complex domains with minimal human supervision. Recently, deep reinforcement learning algorithms combined with self-play have achieved superhuman proficiency in Go, Chess, and Shogi without human data or domain knowledge. In these environments, a reward is always received at the end of the game, however, for many combinatorial optimization environments, rewards are sparse and episodes are not guaranteed to terminate. We introduce Autodidactic Iteration: a novel reinforcement learning algorithm that is able to teach itself how to solve the Rubik's Cube with no human assistance. Our algorithm is able to solve 100{\%} of randomly scrambled cubes while achieving a median solve length of 30 moves -- less than or equal to solvers that employ human domain knowledge.},
archivePrefix = {arXiv},
arxivId = {1805.07470},
author = {McAleer, Stephen and Agostinelli, Forest and Shmakov, Alexander and Baldi, Pierre},
booktitle = {Submitted to Neural Information Processing Systems (NIPS) 2018},
eprint = {1805.07470},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McAleer et al. - 2018 - Solving the Rubik's Cube Without Human Knowledge.pdf:pdf},
month = {may},
title = {{Solving the Rubik's Cube Without Human Knowledge}},
url = {http://arxiv.org/abs/1805.07470},
year = {2018}
}
@incollection{Wu2017,
author = {Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
booktitle = {Advances in Neural Information Processing Systems 30},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2017 - Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation.pdf:pdf},
pages = {5279--5288},
publisher = {Curran Associates, Inc.},
title = {{Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation}},
url = {http://papers.nips.cc/paper/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation.pdf},
year = {2017}
}
@inproceedings{Wang2017,
abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
archivePrefix = {arXiv},
arxivId = {1611.01224},
author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1611.01224},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2016 - Sample Efficient Actor-Critic with Experience Replay.pdf:pdf},
month = {nov},
title = {{Sample Efficient Actor-Critic with Experience Replay}},
url = {http://arxiv.org/abs/1611.01224},
year = {2017}
}
@inproceedings{Ring2011,
author = {Ring, Mark and Schaul, Tom and Schmidhuber, Juergen},
booktitle = {2011 IEEE International Conference on Development and Learning (ICDL)},
doi = {10.1109/DEVLRN.2011.6037326},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ring, Schaul, Schmidhuber - 2011 - The two-dimensional organization of behavior.pdf:pdf},
isbn = {978-1-61284-989-8},
month = {aug},
pages = {1--8},
publisher = {IEEE},
title = {{The two-dimensional organization of behavior}},
url = {http://ieeexplore.ieee.org/document/6037326/},
year = {2011}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
eprint = {1312.5602},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Yang, Cheung - 2015 - A mechanised 3D scanning method for item-level radio frequency identification of palletised products.pdf:pdf},
journal = {NIPS Deep Learning Workshop 2013},
month = {dec},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
month = {jul},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Levine2015,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
eprint = {1504.00702},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levine et al. - 2015 - End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
journal = {The Journal of Machine Learning Research},
month = {apr},
number = {1},
pages = {1334--1373},
publisher = {JMLR.org},
title = {{End-to-End Training of Deep Visuomotor Policies}},
url = {http://arxiv.org/abs/1504.00702},
volume = {17},
year = {2015}
}
@article{Wierstra2014,
author = {Wierstra, Daan and Schaul, Tom and Glasmachers, Tobias and Sun, Yi and Peters, Jan and Schmidhuber, J{\"{u}}rgen},
file = {:home/fabian/Desktop/Natural Evolution Strategies.pdf:pdf},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {black-box optimization,evolution strategies,natural gradient,sampling,stochastic search},
month = {jan},
number = {1},
pages = {949--980},
publisher = {JMLR.org},
title = {{Natural Evolution Strategies}},
url = {http://dl.acm.org/citation.cfm?id=2627435.2638566},
volume = {15},
year = {2014}
}
@article{Tampuu2015,
abstract = {Multiagent systems appear in most social, economical, and political situations. In the present work we extend the Deep Q-Learning Network architecture proposed by Google DeepMind to multiagent environments and investigate how two agents controlled by independent Deep Q-Networks interact in the classic videogame Pong. By manipulating the classical rewarding scheme of Pong we demonstrate how competitive and collaborative behaviors emerge. Competitive agents learn to play and score efficiently. Agents trained under collaborative rewarding schemes find an optimal strategy to keep the ball in the game as long as possible. We also describe the progression from competitive to collaborative behavior. The present work demonstrates that Deep Q-Networks can become a practical tool for studying the decentralized learning of multiagent systems living in highly complex environments.},
archivePrefix = {arXiv},
arxivId = {1511.08779},
author = {Tampuu, Ardi and Matiisen, Tambet and Kodelja, Dorian and Kuzovkin, Ilya and Korjus, Kristjan and Aru, Juhan and Aru, Jaan and Vicente, Raul},
eprint = {1511.08779},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tampuu et al. - 2015 - Multiagent Cooperation and Competition with Deep Reinforcement Learning.pdf:pdf},
month = {nov},
title = {{Multiagent Cooperation and Competition with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1511.08779},
year = {2015}
}
@incollection{Kober2012,
abstract = {As most action generation problems of autonomous robots can be phrased in terms of sequential decision problems, robotics offers a tremendously important and interesting application platform for reinforcement learning. Similarly, the real-world challenges of this domain pose a major real-world check for reinforcement learning. Hence, the interplay between both disciplines can be seen as promising as the one between physics and mathematics. Nevertheless, only a fraction of the scientists working on reinforcement learning are sufficiently tied to robotics to oversee most problems encountered in this context. Thus, we will bring the most important challenges faced by robot reinforcement learning to their attention. To achieve this goal, we will attempt to survey most work that has successfully applied reinforcement learning to behavior generation for real robots. We discuss how the presented successful approaches have been made tractable despite the complexity of the domain and will study how representations or the inclusion of prior knowledge can make a significant difference. As a result, a particular focus of our chapter lies on the choice between model-based and model-free as well as between value function-based and policy search methods. As a result, we obtain a fairly complete survey of robot reinforcement learning which should allow a general reinforcement learning researcher to understand this domain.},
address = {Berlin, Heidelberg},
author = {Kober, Jens and Peters, Jan},
booktitle = {Reinforcement Learning: State-of-the-Art},
doi = {10.1007/978-3-642-27645-3_18},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kober, Peters - 2012 - Reinforcement Learning in Robotics A Survey.pdf:pdf},
isbn = {978-3-642-27645-3},
pages = {579--610},
publisher = {Springer Berlin Heidelberg},
title = {{Reinforcement Learning in Robotics: A Survey}},
url = {https://doi.org/10.1007/978-3-642-27645-3{\_}18},
year = {2012}
}
@article{Legenstein2010,
abstract = {Humans and animals are able to learn complex behaviors based on a massive stream of sensory information from different modalities. Early animal studies have identified learning mechanisms that are based on reward and punishment such that animals tend to avoid actions that lead to punishment whereas rewarded actions are reinforced. However, most algorithms for reward-based learning are only applicable if the dimensionality of the state-space is sufficiently small or its structure is sufficiently simple. Therefore, the question arises how the problem of learning on high-dimensional data is solved in the brain. In this article, we propose a biologically plausible generic two-stage learning system that can directly be applied to raw high-dimensional input streams. The system is composed of a hierarchical slow feature analysis (SFA) network for preprocessing and a simple neural network on top that is trained based on rewards. We demonstrate by computer simulations that this generic architecture is able to learn quite demanding reinforcement learning tasks on high-dimensional visual input streams in a time that is comparable to the time needed when an explicit highly informative low-dimensional state-space representation is given instead of the high-dimensional visual input. The learning speed of the proposed architecture in a task similar to the Morris water maze task is comparable to that found in experimental studies with rats. This study thus supports the hypothesis that slowness learning is one important unsupervised learning principle utilized in the brain to form efficient state representations for behavioral learning.},
author = {Legenstein, Robert and Wilbert, Niko and Wiskott, Laurenz},
doi = {10.1371/journal.pcbi.1000894},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Legenstein, Wilbert, Wiskott - 2010 - Reinforcement Learning on Slow Features of High-Dimensional Input Streams.pdf:pdf},
issn = {1553-7358},
journal = {PLoS Computational Biology},
month = {aug},
number = {8},
pages = {e1000894},
publisher = {Public Library of Science},
title = {{Reinforcement Learning on Slow Features of High-Dimensional Input Streams}},
url = {https://dx.plos.org/10.1371/journal.pcbi.1000894},
volume = {6},
year = {2010}
}
@inproceedings{10.1007/978-3-030-01054-6_1,
abstract = {ViZDoom is a robust, first-person shooter reinforcement learning environment, characterized by a significant degree of latent state information. In this paper, double-Q learning and prioritized experience replay methods are tested under a certain ViZDoom combat scenario using a competitive deep recurrent Q-network (DRQN) architecture. In addition, an ensembling technique known as snapshot ensembling is employed using a specific annealed learning rate to observe differences in ensembling efficacy under these two methods. Annealed learning rates are important in general to the training of deep neural network models, as they shake up the status-quo and counter a model's tending towards local optima. While both variants show performance exceeding those of built-in AI agents of the game, the known stabilizing effects of double-Q learning are illustrated, and priority experience replay is again validated in its usefulness by showing immediate results early on in agent development, with the caveat that value overestimation is accelerated in this case. In addition, some unique behaviors are observed to develop for priority experience replay (PER) and double-Q (DDQ) variants, and snapshot ensembling of both PER and DDQ proves a valuable method for improving performance of the ViZDoom Marine.},
address = {Cham},
author = {Schulze, Christopher and Schulze, Marcus},
booktitle = {Intelligent Systems and Applications},
file = {:home/fabian/Desktop/ViZDoom$\backslash$: DRQN with Prioritized Experience Replay, Double-Q Learning, {\&} Snapshot Ensembling.pdf:pdf},
isbn = {978-3-030-01054-6},
pages = {1--17},
publisher = {Springer International Publishing},
title = {{ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning and Snapshot Ensembling}},
year = {2019}
}
@inproceedings{VanHasselt2016,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
address = {Phoenix, Arizona},
archivePrefix = {arXiv},
arxivId = {1509.06461},
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
booktitle = {AAAI Conference on Artificial Intelligence},
eprint = {1509.06461},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van Hasselt, Guez, Silver - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:pdf},
month = {sep},
pages = {2094--2100},
title = {{Deep Reinforcement Learning with Double Q-learning}},
url = {http://arxiv.org/abs/1509.06461},
year = {2016}
}
@article{Martens2015,
abstract = {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-Factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC's approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.},
archivePrefix = {arXiv},
arxivId = {1503.05671},
author = {Martens, James and Grosse, Roger},
eprint = {1503.05671},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens, Grosse - 2015 - Optimizing Neural Networks with Kronecker-factored Approximate Curvature.pdf:pdf},
month = {mar},
title = {{Optimizing Neural Networks with Kronecker-factored Approximate Curvature}},
url = {http://arxiv.org/abs/1503.05671},
year = {2015}
}
@inproceedings{Yahya2016,
abstract = {In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.},
archivePrefix = {arXiv},
arxivId = {1610.00673},
author = {Yahya, Ali and Li, Adrian and Kalakrishnan, Mrinal and Chebotar, Yevgen and Levine, Sergey},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
eprint = {1610.00673},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yahya et al. - 2016 - Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.pdf:pdf},
month = {oct},
pages = {79--86},
title = {{Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search}},
url = {http://arxiv.org/abs/1610.00673},
year = {2016}
}
@incollection{Chua2018,
author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems 31},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chua et al. - 2018 - Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models.pdf:pdf},
pages = {4759--4770},
publisher = {Curran Associates, Inc.},
title = {{Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models}},
url = {http://papers.nips.cc/paper/7725-deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models.pdf},
year = {2018}
}
@inproceedings{Tang2018,
abstract = {Developing agents to engage in complex goal-oriented dialogues is challenging partly because the main learning signals are very sparse in long conversations. In this paper, we propose a divide-and-conquer approach that discovers and exploits the hidden structure of the task to enable efficient policy learning. First, given successful example dialogues, we propose the Subgoal Discovery Network (SDN) to divide a complex goal-oriented task into a set of simpler subgoals in an unsupervised fashion. We then use these subgoals to learn a multi-level policy by hierarchical reinforcement learning. We demonstrate our method by building a dialogue agent for the composite task of travel planning. Experiments with simulated and real users show that our approach performs competitively against a state-of-the-art method that requires human-defined subgoals. Moreover, we show that the learned subgoals are often human comprehensible.},
address = {Brussels, Belgium },
archivePrefix = {arXiv},
arxivId = {1804.07855},
author = {Tang, Da and Li, Xiujun and Gao, Jianfeng and Wang, Chong and Li, Lihong and Jebara, Tony},
booktitle = {2018 Conference on Empirical Methods in Natural Language Processing },
eprint = {1804.07855},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2018 - Subgoal Discovery for Hierarchical Dialogue Policy Learning.pdf:pdf},
month = {apr},
pages = {2298--2309 },
publisher = {Association for Computational Linguistics },
title = {{Subgoal Discovery for Hierarchical Dialogue Policy Learning}},
url = {http://arxiv.org/abs/1804.07855},
year = {2018}
}
@inproceedings{Hanson1993,
abstract = {Papers from the 6th Conference on Neural Information Processing Systems--Natural and Synthetic, Denver, 1992. Cf. Pref.},
author = {Hanson, Stephen José. and Cowan, J. D. (Jack D.) and Giles, C. Lee.},
booktitle = {Advances in Neural Information Processing Systems (NIPS) 5},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hanson, Cowan, Giles - 1993 - Advances in neural information processing systems 5.pdf:pdf},
isbn = {1558602747},
pages = {1049},
publisher = {Morgan Kaufmann},
title = {{Learning Control Under Extreme Uncertainty}},
url = {https://dl.acm.org/citation.cfm?id=668222},
year = {1993}
}
@incollection{Heess2015,
author = {Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/fabian/Desktop/Learning Continuous Control Policies by Stochastic Value Gradients.pdf:pdf},
pages = {2944--2952},
publisher = {Curran Associates, Inc.},
title = {{Learning Continuous Control Policies by Stochastic Value Gradients}},
url = {http://papers.nips.cc/paper/5796-learning-continuous-control-policies-by-stochastic-value-gradients.pdf},
year = {2015}
}
@article{Luciw2013,
abstract = {Curiosity Driven Modular Incremental Slow Feature Analysis (CD-MISFA;) is a recently introduced model of intrinsically-motivated invariance learning. Artificial curiosity enables the orderly formation of multiple stable sensory representations to simplify the agent's complex sensory input. We discuss computational properties of the CD-MISFA model itself as well as neurophysiological analogs fulfilling similar functional roles. CD-MISFA combines 1. unsupervised representation learning through the slowness principle, 2. generation of an intrinsic reward signal through learning progress of the developing features, and 3. balancing of exploration and exploitation to maximize learning progress and quickly learn multiple feature sets for perceptual simplification. Experimental results on synthetic observations and on the iCub robot show that the intrinsic value system is essential for representation learning. Representations are typically explored and learned in order from least to most costly, as predicted by the theory of curiosity.},
author = {Luciw, Matthew and Kompella, Varun and Kazerounian, Sohrob and Schmidhuber, Juergen},
doi = {10.3389/fnbot.2013.00009},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luciw et al. - 2013 - An intrinsic value system for developing multiple invariant representations with incremental slowness learning.pdf:pdf},
issn = {1662-5218},
journal = {Frontiers in neurorobotics},
keywords = {exploration-exploitation,intrinsic motivation systems,neuromodulation,norepinephrine,slow feature analysis},
pages = {9},
pmid = {23755011},
publisher = {Frontiers Media SA},
title = {{An intrinsic value system for developing multiple invariant representations with incremental slowness learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23755011 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3667249},
volume = {7},
year = {2013}
}
@inproceedings{Schmidhuber1991,
abstract = {{A novel curious model-building control system is described which actively tries to provoke situations for which it learned to expect to learn something about the environment. Such a system has been implemented as a four-network system based on Watkins' Q-learning algorithm which can be used to maximize the expectation of the temporal derivative of the adaptive assumed reliability of future predictions. An experiment with an artificial nondeterministic environment demonstrates that the system can be superior to previous model-building control systems, which do not address the problem of modeling the reliability of the world model's predictions in uncertain environments and use ad-hoc methods (like random search) to train the world model.{\textless}{\textless}ETX{\textgreater}{\textgreater}{\}}, keywords={\{}adaptive control;adaptive systems;learning systems;neural nets;adaptive control;adaptive systems;learning systems;model-building control systems;four-network system;Watkins' Q-learning algorithm;temporal derivative;adaptive assumed reliability;future predictions;artificial nondeterministic environment;Control system synthesis;Predictive models;Adaptive control;Programmable control;Learning;Computer science;Manipulator dynamics;Computer architecture;Computer networks;Error correction}},
author = {Schmidhuber, J.},
booktitle = {1991 IEEE International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.1991.170605},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 1991 - Curious model-building control systems.pdf:pdf},
isbn = {0-7803-0227-3},
pages = {1458--1463},
publisher = {IEEE},
title = {{Curious model-building control systems}},
url = {http://ieeexplore.ieee.org/document/170605/},
year = {1991}
}
@article{Aytar2018,
abstract = {Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.},
archivePrefix = {arXiv},
arxivId = {1805.11592},
author = {Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Tom Le and Wang, Ziyu and de Freitas, Nando},
eprint = {1805.11592},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aytar et al. - 2018 - Playing hard exploration games by watching YouTube.pdf:pdf},
month = {may},
title = {{Playing hard exploration games by watching YouTube}},
url = {http://arxiv.org/abs/1805.11592},
year = {2018}
}
@inproceedings{Duan2017,
abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at https://bit.ly/nips2017-oneshot .},
archivePrefix = {arXiv},
arxivId = {1703.07326},
author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C. and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
booktitle = {31st Conference on Advances in Neural Information Processing Systems (NIPS)},
eprint = {1703.07326},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duan et al. - 2017 - One-Shot Imitation Learning.pdf:pdf},
month = {mar},
pages = {1087--1098},
title = {{One-Shot Imitation Learning}},
url = {http://arxiv.org/abs/1703.07326},
year = {2017}
}
@inproceedings{Riedmiller2005,
abstract = {This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
address = {Berlin, Heidelberg},
author = {Riedmiller, Martin},
booktitle = {Machine Learning: ECML 2005},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riedmiller - 2005 - Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method.pdf:pdf},
isbn = {978-3-540-31692-3},
pages = {317--328},
publisher = {Springer Berlin Heidelberg},
title = {{Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method}},
year = {2005}
}
@inproceedings{Fujimoto2018,
abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
archivePrefix = {arXiv},
arxivId = {1802.09477},
author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
booktitle = {International Conference for Machine Learning },
eprint = {1802.09477},
file = {:home/fabian/Desktop/Addressing Function Approximation Error in Actor-Critic Methods.pdf:pdf},
month = {feb},
title = {{Addressing Function Approximation Error in Actor-Critic Methods}},
url = {http://arxiv.org/abs/1802.09477},
year = {2018}
}
@book{Miikkulainen2002,
address = {San Francisco},
author = {Miikkulainen, Risto and Stanley, Kenneth O.},
booktitle = {Genetic and Evolutionary Computation Conference (GECCO-2002)},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miikkulainen, Stanley - 2002 - Efficient Reinforcement Learning through Evolving Neural Network Topologies.pdf:pdf},
isbn = {1558608788},
pages = {9},
publisher = {Morgan Kaufmann},
title = {{Efficient Reinforcement Learning through Evolving Neural Network Topologies}},
url = {http://nn.cs.utexas.edu/?stanley:gecco02b},
year = {2002}
}
@inproceedings{Lange2012,
abstract = {We propose a learning architecture, that is able to do reinforcement learning based on raw visual input data. In contrast to previous approaches, not only the control policy is learned. In order to be successful, the system must also autonomously learn, how to extract relevant information out of a high-dimensional stream of input information, for which the semantics are not provided to the learning system. We give a first proof-of-concept of this novel learning architecture on a challenging benchmark, namely visual control of a racing slot car. The resulting policy, learned only by success or failure, is hardly beaten by an experienced human player},
author = {Lange, Sascha and Riedmiller, Martin and Voigtlander, Arne},
booktitle = {2012 Int. Jt. Conf. Neural Networks},
doi = {10.1109/IJCNN.2012.6252823},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lange, Riedmiller, Voigtlander - 2012 - Autonomous reinforcement learning on raw visual input data in a real world application.pdf:pdf},
isbn = {978-1-4673-1490-9},
month = {jun},
pages = {1--8},
publisher = {IEEE},
title = {{Autonomous reinforcement learning on raw visual input data in a real world application}},
url = {http://ieeexplore.ieee.org/document/6252823/},
year = {2012}
}
@incollection{Lowe2017,
address = {Long Beach, CA, USA},
annote = {MADDPG base paper},
author = {Lowe, Ryan and WU, Y I and Tamar, Aviv and Harb, Jean and {Pieter Abbeel}, OpenAI and Mordatch, Igor},
booktitle = {Conference on Neural Information Processing Systems (NIPS)},
file = {:home/fabian/Desktop/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf:pdf},
pages = {6379--6390},
publisher = {Curran Associates, Inc.},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}},
url = {http://papers.nips.cc/paper/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf},
year = {2017}
}
@inproceedings{10.1007/978-3-319-09165-5_6,
abstract = {In recent years the Monte Carlo tree search revolution has spread from computer Go to many areas, including computer Hex. MCTS-based Hex players now outperform traditional knowledge-based alpha-beta search players, and the reigning Computer Olympiad Hex gold medallist is the MCTS player MoHex. In this paper we show how to strengthen MoHex, and observe that---as in computer Go---using learned patterns in priors and replacing a hand-crafted simulation policy by a softmax policy that uses learned patterns significantly increases playing strength. The result is MoHex 2.0, about 250 Elo points stronger than MoHex on the 11{\$}{\$}$\backslash$times {\$}{\$}{\{}$\backslash$texttimes{\}}11 board, and 300 Elo points stronger on the 13{\$}{\$}$\backslash$times {\$}{\$}{\{}$\backslash$texttimes{\}}13 board.},
address = {Cham},
author = {Huang, Shih-Chieh and Arneson, Broderick and Hayward, Ryan B and M{\"{u}}ller, Martin and Pawlewicz, Jakub},
booktitle = {Computers and Games},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2014 - MoHex 2.0 A Pattern-Based MCTS Hex Player.pdf:pdf},
isbn = {978-3-319-09165-5},
pages = {60--71},
publisher = {Springer International Publishing},
title = {{MoHex 2.0: A Pattern-Based MCTS Hex Player}},
year = {2014}
}
@article{Lin1992,
abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
annote = {replay memory basic paper --{\textgreater} off policy},
author = {Lin, Long-Ji},
doi = {10.1007/BF00992699},
issn = {1573-0565},
journal = {Machine Learning},
month = {may},
number = {3},
pages = {293--321},
title = {{Self-improving reactive agents based on reinforcement learning, planning and teaching}},
url = {https://doi.org/10.1007/BF00992699},
volume = {8},
year = {1992}
}
@article{Hunter2004,
author = {Hunter, David R and Lange, Kenneth},
journal = {The American Statistician},
number = {1},
pages = {30--37},
title = {{A tutorial on MM algorithms}},
volume = {58},
year = {2004}
}
@article{Williams1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {Williams, Ronald J},
doi = {10.1007/BF00992696},
issn = {1573-0565},
journal = {Machine Learning},
month = {may},
number = {3},
pages = {229--256},
title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
url = {https://doi.org/10.1007/BF00992696},
volume = {8},
year = {1992}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf:pdf},
journal = {IEEE Signal Processing Magazine, Special Issue on Deep Learning for Image Understanding},
month = {aug},
title = {{A Brief Survey of Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.05866},
year = {2017}
}
@inproceedings{Peng2016,
author = {Peng, Xue Bin and Berseth, Glen and van de Panne, Michiel},
booktitle = {ACM Transactions on Graphics},
doi = {10.1145/2897824.2925881},
file = {:home/fabian/Desktop/Terrain-adaptive locomotion skills using deep reinforcement learning.pdf:pdf},
issn = {07300301},
keywords = {physics-based characters,reinforcement learning},
month = {jul},
number = {4},
pages = {1--12},
publisher = {ACM},
title = {{Terrain-adaptive locomotion skills using deep reinforcement learning}},
url = {http://dl.acm.org/citation.cfm?doid=2897824.2925881},
volume = {35},
year = {2016}
}
@inproceedings{Horgan2018,
abstract = {We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.},
annote = {Mutliple actors for DQN and DDPG 
Apex structure},
archivePrefix = {arXiv},
arxivId = {1803.00933},
author = {Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and van Hasselt, Hado and Silver, David},
booktitle = {nternational Conference on Learning Representations},
eprint = {1803.00933},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horgan et al. - 2018 - Distributed Prioritized Experience Replay.pdf:pdf},
month = {mar},
title = {{Distributed Prioritized Experience Replay}},
url = {http://arxiv.org/abs/1803.00933},
year = {2018}
}
@inproceedings{Ioffe2015,
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {448--456},
publisher = {JMLR.org},
series = {ICML'15},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
year = {2015}
}
@article{Crites1996,
abstract = {This paper describes the application of reinforcement learning (RL) to the difficult real world problem of elevator dispatching. The elevator domain poses a combination of challenges not seen in most RL research to date. Elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems. Their states are not fully observable and they are nonstationary due to changing passenger arrival rates. In addition, we use a team of RL agents, each of which is responsible for controlling one elevator car. The team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state. In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware. These results demonstrate the power of RL on a very large scale stochastic dynamic optimiz...},
author = {Crites, Robert and Crites, Robert and Barto, Andrew},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Crites, Crites, Barto - 1996 - Improving Elevator Performance Using Reinforcement Learning.pdf:pdf},
journal = {ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8},
pages = {1017----1023},
title = {{Improving Elevator Performance Using Reinforcement Learning}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.5519},
volume = {8},
year = {1996}
}
@inproceedings{Tammelin2015,
abstract = {Cepheus is the first computer program to essentially solve a game of imperfect information that is played competitively by humans. The game it plays is heads-up limit Texas hold'em poker, a game with over 10 14 information sets, and a challenge problem for artificial intelligence for over 10 years. Cepheus was trained using a new variant of Counterfactual Regret Minimization (CFR), called CFR + , using 4800 CPUs running for 68 days. In this paper we describe in detail the engineering details required to make this computation a reality. We also prove the theoretical sound-ness of CFR + and its component algorithm, regret-matching +. We further give a hint towards understanding the success of CFR + by proving a tracking regret bound for this new regret matching algorithm. We present results showing the role of the al-gorithmic components and the engineering choices to the success of CFR + .},
address = {Buenos Aires, Argentina},
author = {Tammelin, Oskari and Burch, Neil and Johanson, Michael and Bowling, Michael},
booktitle = {IJCAI'15 Proceedings of the 24th International Conference on Artificial Intelligence},
file = {:home/fabian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tammelin et al. - 2015 - Solving Heads-up Limit Texas Hold'em.pdf:pdf},
pages = {645--652 },
publisher = {AAAI Press},
title = {{Solving Heads-up Limit Texas Hold'em}},
url = {https://dl.acm.org/citation.cfm?id=2832339},
year = {2015}
}
